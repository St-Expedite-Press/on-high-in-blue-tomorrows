{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation (Otsu luma, SageMaker-style: read-only input, write-only output)\n",
    "\n",
    "This notebook is designed to run on local CPU while matching the SageMaker mental model:\n",
    "\n",
    "- Treat `INPUT_ROOT` as **read-only**.\n",
    "- Write all new artifacts under `OUTPUT_ROOT`.\n",
    "- Use shard parameters so the same code can scale to SageMaker Processing.\n",
    "\n",
    "## Input criteria (must already be true)\n",
    "\n",
    "`INPUT_ROOT/` must contain:\n",
    "\n",
    "- `plates_structured/` with exactly 435 plate directories named `plate-###`\n",
    "- For each plate directory:\n",
    "  - `manifest.json`\n",
    "  - `source/?` with the immutable source image referenced by `manifest.json.source_image`\n",
    "- `schemas/plate.manifest.schema.json`\n",
    "- `schemas/run.manifest.schema.json`\n",
    "\n",
    "## Midpoint artifacts (written once)\n",
    "\n",
    "- `OUTPUT_ROOT/schemas/segmentation.otsu.schema.json`\n",
    "- `OUTPUT_ROOT/reports/<run_id>/report.json`\n",
    "\n",
    "## Outputs (append-only, per plate)\n",
    "\n",
    "For each plate in the shard:\n",
    "\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/metrics.json`\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/segmentation.json`\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/segmentation_mask.png`\n",
    "\n",
    "## Constraints enforced\n",
    "\n",
    "- No mutation of `INPUT_ROOT`.\n",
    "- Append-only run outputs in `OUTPUT_ROOT`.\n",
    "- `metrics.json` validates against `schemas/run.manifest.schema.json`.\n",
    "- `segmentation.json` validates against `schemas/segmentation.otsu.schema.json` (written by this notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from jsonschema import Draft202012Validator\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "DEFAULT_MAX_DIM = 1024\n",
    "\n",
    "def utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def generate_run_id(models: list[str], note: str) -> str:\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    payload = \"|\".join(models) + \"|\" + note\n",
    "    short = hashlib.sha1(payload.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"run-{stamp}-{short}\"\n",
    "\n",
    "def load_validator(path: Path) -> Draft202012Validator:\n",
    "    schema = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    Draft202012Validator.check_schema(schema)\n",
    "    return Draft202012Validator(schema)\n",
    "\n",
    "def resize_fit(img: Image.Image, max_dim: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    m = max(w, h)\n",
    "    if m <= max_dim:\n",
    "        return img\n",
    "    s = max_dim / m\n",
    "    return img.resize((max(1, int(round(w * s))), max(1, int(round(h * s)))), resample=Image.BILINEAR)\n",
    "\n",
    "def otsu_threshold(hist: list[int]) -> int:\n",
    "    total = float(sum(hist))\n",
    "    if total <= 0:\n",
    "        return 127\n",
    "\n",
    "    sum_total = 0.0\n",
    "    for i, c in enumerate(hist):\n",
    "        sum_total += i * float(c)\n",
    "\n",
    "    sum_b = 0.0\n",
    "    w_b = 0.0\n",
    "    var_max = -1.0\n",
    "    threshold = 127\n",
    "\n",
    "    for t in range(256):\n",
    "        c = float(hist[t])\n",
    "        w_b += c\n",
    "        if w_b <= 0:\n",
    "            continue\n",
    "        w_f = total - w_b\n",
    "        if w_f <= 0:\n",
    "            break\n",
    "        sum_b += float(t) * c\n",
    "        m_b = sum_b / w_b\n",
    "        m_f = (sum_total - sum_b) / w_f\n",
    "        var_between = w_b * w_f * (m_b - m_f) ** 2\n",
    "        if var_between > var_max:\n",
    "            var_max = var_between\n",
    "            threshold = t\n",
    "\n",
    "    return int(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_input_root(start: Path) -> Path:\n",
    "    override = os.environ.get(\"BWS_INPUT_ROOT\")\n",
    "    if override:\n",
    "        p = Path(override).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise RuntimeError(f\"BWS_INPUT_ROOT does not exist: {p}\")\n",
    "\n",
    "    def is_scaffolded(p: Path) -> bool:\n",
    "        if not p.is_dir():\n",
    "            return False\n",
    "        if not (p / \"plates_structured\").exists():\n",
    "            return False\n",
    "        if not (p / \"schemas\" / \"plate.manifest.schema.json\").exists():\n",
    "            return False\n",
    "        if not (p / \"schemas\" / \"run.manifest.schema.json\").exists():\n",
    "            return False\n",
    "        if not (p / \"data.json\").exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    candidates = [start] + list(start.parents)\n",
    "    for base in candidates:\n",
    "        if is_scaffolded(base):\n",
    "            return base\n",
    "        try:\n",
    "            for child in sorted(base.iterdir()):\n",
    "                if is_scaffolded(child):\n",
    "                    return child\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise RuntimeError(\"Could not find scaffolded INPUT_ROOT; set BWS_INPUT_ROOT\")\n",
    "\n",
    "INPUT_ROOT = find_input_root(Path.cwd())\n",
    "OUTPUT_ROOT = Path(os.environ.get(\"BWS_OUTPUT_ROOT\", str(INPUT_ROOT / \"_RUN_OUTPUT\"))).expanduser().resolve()\n",
    "SHARD_INDEX = int(os.environ.get(\"BWS_SHARD_INDEX\", \"0\"))\n",
    "SHARD_COUNT = int(os.environ.get(\"BWS_SHARD_COUNT\", \"1\"))\n",
    "RUN_ID = os.environ.get(\"BWS_RUN_ID\", None)\n",
    "SKIP_IF_PRESENT = os.environ.get(\"BWS_SKIP_IF_PRESENT\", \"1\") == \"1\"\n",
    "MAX_DIM = int(os.environ.get(\"BWS_MAX_DIM\", str(DEFAULT_MAX_DIM)))\n",
    "\n",
    "print(\"INPUT_ROOT :\", INPUT_ROOT)\n",
    "print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n",
    "print(\"SHARD      :\", SHARD_INDEX, \"/\", SHARD_COUNT)\n",
    "print(\"RUN_ID     :\", RUN_ID)\n",
    "print(\"SKIP       :\", SKIP_IF_PRESENT)\n",
    "print(\"MAX_DIM    :\", MAX_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "PLATES_ROOT = INPUT_ROOT / \"plates_structured\"\n",
    "SCHEMAS_ROOT = INPUT_ROOT / \"schemas\"\n",
    "\n",
    "plate_schema_path = SCHEMAS_ROOT / \"plate.manifest.schema.json\"\n",
    "run_schema_path = SCHEMAS_ROOT / \"run.manifest.schema.json\"\n",
    "\n",
    "if not plate_schema_path.exists():\n",
    "    raise RuntimeError(f\"Missing: {plate_schema_path}\")\n",
    "if not run_schema_path.exists():\n",
    "    raise RuntimeError(f\"Missing: {run_schema_path}\")\n",
    "\n",
    "plate_validator = load_validator(plate_schema_path)\n",
    "run_validator = load_validator(run_schema_path)\n",
    "\n",
    "plates = sorted([p for p in PLATES_ROOT.iterdir() if p.is_dir() and p.name.startswith(\"plate-\")])\n",
    "if len(plates) != 435:\n",
    "    raise RuntimeError(f\"Unexpected plate count: {len(plates)}\")\n",
    "\n",
    "selected = [p for i, p in enumerate(plates) if i % SHARD_COUNT == SHARD_INDEX]\n",
    "print(\"plates_total   :\", len(plates))\n",
    "print(\"plates_selected:\", len(selected))\n",
    "\n",
    "schema_dir = OUTPUT_ROOT / \"schemas\"\n",
    "schema_dir.mkdir(parents=True, exist_ok=True)\n",
    "seg_schema_path = schema_dir / \"segmentation.otsu.schema.json\"\n",
    "\n",
    "SEGMENTATION_SCHEMA = {\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"https://burning-world-series/schemas/segmentation.otsu.schema.json\",\n",
    "    \"title\": \"Segmentation (Otsu Luma, v1)\",\n",
    "    \"type\": \"object\",\n",
    "    \"additionalProperties\": False,\n",
    "    \"required\": [\n",
    "        \"plate_id\",\n",
    "        \"run_id\",\n",
    "        \"timestamp\",\n",
    "        \"source_image\",\n",
    "        \"method\",\n",
    "        \"params\",\n",
    "        \"original_geometry\",\n",
    "        \"mask_geometry\",\n",
    "        \"outputs\",\n",
    "        \"foreground_ratio\",\n",
    "    ],\n",
    "    \"properties\": {\n",
    "        \"plate_id\": {\"type\": \"string\"},\n",
    "        \"run_id\": {\"type\": \"string\"},\n",
    "        \"timestamp\": {\"type\": \"string\"},\n",
    "        \"source_image\": {\"type\": \"string\"},\n",
    "        \"method\": {\"type\": \"string\"},\n",
    "        \"params\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": True,\n",
    "            \"required\": [\"max_dim\", \"threshold\", \"polarity\"],\n",
    "            \"properties\": {\n",
    "                \"max_dim\": {\"type\": \"integer\"},\n",
    "                \"threshold\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 255},\n",
    "                \"polarity\": {\"type\": \"string\"},\n",
    "            },\n",
    "        },\n",
    "        \"original_geometry\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"required\": [\"width_px\", \"height_px\"],\n",
    "            \"properties\": {\n",
    "                \"width_px\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                \"height_px\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "            },\n",
    "        },\n",
    "        \"mask_geometry\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"required\": [\"width_px\", \"height_px\"],\n",
    "            \"properties\": {\n",
    "                \"width_px\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                \"height_px\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "            },\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"required\": [\"mask_png\"],\n",
    "            \"properties\": {\"mask_png\": {\"type\": \"string\"}},\n",
    "        },\n",
    "        \"foreground_ratio\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
    "    },\n",
    "}\n",
    "\n",
    "seg_schema_path.write_text(json.dumps(SEGMENTATION_SCHEMA, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "seg_validator = load_validator(seg_schema_path)\n",
    "print(\"wrote schema:\", seg_schema_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODELS = [f\"segmentation-otsu-luma-v1(max_dim={MAX_DIM})\"]\n",
    "NOTE = \"cheap luma otsu threshold segmentation (downsampled mask)\"\n",
    "RUN_ID_EFFECTIVE = RUN_ID or generate_run_id(MODELS, NOTE)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID_EFFECTIVE,\n",
    "    \"timestamp\": utc_iso(),\n",
    "    \"dataset_root\": str(INPUT_ROOT),\n",
    "    \"input_root\": str(INPUT_ROOT),\n",
    "    \"output_root\": str(OUTPUT_ROOT),\n",
    "    \"shard_index\": SHARD_INDEX,\n",
    "    \"shard_count\": SHARD_COUNT,\n",
    "    \"plates_total\": len(plates),\n",
    "    \"plates_selected\": len(selected),\n",
    "    \"plates_processed\": 0,\n",
    "    \"plates_skipped\": 0,\n",
    "    \"decode_failures\": 0,\n",
    "    \"schema_failures\": 0,\n",
    "    \"errors_sample\": [],\n",
    "}\n",
    "\n",
    "def compute_segmentation(plate_dir: Path) -> tuple[dict, dict, Image.Image]:\n",
    "    manifest = json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    errs = list(plate_validator.iter_errors(manifest))\n",
    "    if errs:\n",
    "        raise RuntimeError(f\"plate manifest schema error: {list(errs[0].path)} -> {errs[0].message}\")\n",
    "\n",
    "    src_path = plate_dir / manifest[\"source_image\"]\n",
    "    if not src_path.exists():\n",
    "        raise RuntimeError(\"missing source image\")\n",
    "\n",
    "    run_manifest = {\n",
    "        \"run_id\": RUN_ID_EFFECTIVE,\n",
    "        \"plate_id\": plate_dir.name,\n",
    "        \"timestamp\": utc_iso(),\n",
    "        \"models\": MODELS,\n",
    "        \"outputs\": [\"segmentation.json\", \"segmentation_mask.png\"],\n",
    "        \"notes\": NOTE,\n",
    "    }\n",
    "\n",
    "    rerrs = list(run_validator.iter_errors(run_manifest))\n",
    "    if rerrs:\n",
    "        raise RuntimeError(f\"run manifest schema error: {list(rerrs[0].path)} -> {rerrs[0].message}\")\n",
    "\n",
    "    with Image.open(src_path) as img:\n",
    "        w0, h0 = img.size\n",
    "        L = img.convert(\"L\")\n",
    "        Ls = resize_fit(L, MAX_DIM)\n",
    "        ws, hs = Ls.size\n",
    "\n",
    "        hist = Ls.histogram()[:256]\n",
    "        thr = otsu_threshold(hist)\n",
    "        arr = np.asarray(Ls, dtype=np.uint8)\n",
    "        mask_arr = (arr < thr).astype(np.uint8) * 255\n",
    "        mask_img = Image.fromarray(mask_arr, mode=\"L\")\n",
    "\n",
    "    foreground_ratio = float(mask_arr.mean() / 255.0)\n",
    "\n",
    "    seg = {\n",
    "        \"plate_id\": manifest[\"plate_id\"],\n",
    "        \"run_id\": RUN_ID_EFFECTIVE,\n",
    "        \"timestamp\": utc_iso(),\n",
    "        \"source_image\": manifest[\"source_image\"],\n",
    "        \"method\": \"otsu-luma-v1\",\n",
    "        \"params\": {\"max_dim\": int(MAX_DIM), \"threshold\": int(thr), \"polarity\": \"dark-is-foreground\"},\n",
    "        \"original_geometry\": {\"width_px\": int(w0), \"height_px\": int(h0)},\n",
    "        \"mask_geometry\": {\"width_px\": int(ws), \"height_px\": int(hs)},\n",
    "        \"outputs\": {\"mask_png\": \"segmentation_mask.png\"},\n",
    "        \"foreground_ratio\": foreground_ratio,\n",
    "    }\n",
    "\n",
    "    berrs = list(seg_validator.iter_errors(seg))\n",
    "    if berrs:\n",
    "        raise RuntimeError(f\"segmentation schema error: {list(berrs[0].path)} -> {berrs[0].message}\")\n",
    "\n",
    "    return run_manifest, seg, mask_img\n",
    "\n",
    "for plate_dir in tqdm(selected, desc=\"plates\"):\n",
    "    out_plate_dir = OUTPUT_ROOT / \"plates_structured\" / plate_dir.name\n",
    "    out_run_dir = out_plate_dir / \"runs\" / RUN_ID_EFFECTIVE\n",
    "    out_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_metrics = out_run_dir / \"metrics.json\"\n",
    "    out_seg = out_run_dir / \"segmentation.json\"\n",
    "    out_mask = out_run_dir / \"segmentation_mask.png\"\n",
    "\n",
    "    if SKIP_IF_PRESENT and out_metrics.exists() and out_seg.exists() and out_mask.exists():\n",
    "        report[\"plates_skipped\"] += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        run_manifest, seg, mask_img = compute_segmentation(plate_dir)\n",
    "        out_metrics.write_text(json.dumps(run_manifest, indent=2), encoding=\"utf-8\")\n",
    "        out_seg.write_text(json.dumps(seg, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "        mask_img.save(out_mask, format=\"PNG\", optimize=True)\n",
    "        report[\"plates_processed\"] += 1\n",
    "    except Exception as e:\n",
    "        report[\"decode_failures\"] += 1\n",
    "        if len(report[\"errors_sample\"]) < 10:\n",
    "            report[\"errors_sample\"].append(f\"{plate_dir.name}: {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "report_dir = OUTPUT_ROOT / \"reports\" / RUN_ID_EFFECTIVE\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "(report_dir / \"report.json\").write_text(json.dumps(report, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
