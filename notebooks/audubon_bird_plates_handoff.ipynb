{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b0ff19",
   "metadata": {},
   "source": [
    "# Audubon Bird Plates - Handoff + Contract Assertions\n",
    "\n",
    "Use this notebook as the entrypoint once a dataset has been scaffolded (i.e., `plates_structured/`, schemas, checksums, ledger).\n",
    "\n",
    "Dataset root resolution:\n",
    "- Colab: mounts Drive and searches under `/content/drive/MyDrive/burning-world-series/`\n",
    "- Local: searches upward from the current working directory for a folder starting with `audubon-bird-plates`\n",
    "- Override: set `BWS_DATASET_ROOT` to an explicit path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b6eb6",
   "metadata": {},
   "source": [
    "## Handoff: load project context (read-only)\n",
    "\n",
    "Purpose:\n",
    "- Resolve dataset root\n",
    "- Expose canonical paths\n",
    "- Perform light sanity checks only\n",
    "\n",
    "This cell must:\n",
    "- Not write files\n",
    "- Not validate schemas deeply\n",
    "- Not compute checksums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_dataset_root(start: Path) -> Path:\n",
    "    override = os.environ.get(\"BWS_DATASET_ROOT\")\n",
    "    if override:\n",
    "        p = Path(override).expanduser()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise RuntimeError(f\"BWS_DATASET_ROOT does not exist: {p}\")\n",
    "\n",
    "    def is_dataset_dir(p: Path) -> bool:\n",
    "        if not p.is_dir():\n",
    "            return False\n",
    "        if not p.name.startswith(\"audubon-bird-plates\"):\n",
    "            return False\n",
    "        if not (p / \"data.json\").exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    if is_dataset_dir(start):\n",
    "        return start\n",
    "\n",
    "    for base in [start] + list(start.parents):\n",
    "        try:\n",
    "            children = list(base.iterdir())\n",
    "        except Exception:\n",
    "            continue\n",
    "        for child in sorted(children):\n",
    "            if is_dataset_dir(child):\n",
    "                return child\n",
    "\n",
    "    raise RuntimeError(\"Could not find dataset root; set BWS_DATASET_ROOT\")\n",
    "\n",
    "if in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    project_root = Path(\"/content/drive/MyDrive/burning-world-series\")\n",
    "    assert project_root.exists(), \"Project root missing\"\n",
    "    DATASET_ROOT = find_dataset_root(project_root)\n",
    "else:\n",
    "    DATASET_ROOT = find_dataset_root(Path.cwd())\n",
    "\n",
    "PLATES_ORIGINAL = DATASET_ROOT / \"plates\"\n",
    "PLATES_STRUCTURED = DATASET_ROOT / \"plates_structured\"\n",
    "SCHEMA_DIR = DATASET_ROOT / \"schemas\"\n",
    "LEDGER_DIR = DATASET_ROOT / \"ledger\"\n",
    "DATA_JSON = DATASET_ROOT / \"data.json\"\n",
    "README_MD = DATASET_ROOT / \"README.md\"\n",
    "\n",
    "assert PLATES_STRUCTURED.exists(), \"plates_structured missing (run audubon_bird_plates_setup.ipynb first)\"\n",
    "assert SCHEMA_DIR.exists(), \"schemas missing\"\n",
    "assert LEDGER_DIR.exists(), \"ledger missing\"\n",
    "assert DATA_JSON.exists(), \"data.json missing\"\n",
    "\n",
    "plate_dirs = [p for p in PLATES_STRUCTURED.iterdir() if p.is_dir()]\n",
    "assert len(plate_dirs) == 435, \"Unexpected plate count\"\n",
    "\n",
    "def get_plate_dir(plate_number: int) -> Path:\n",
    "    return PLATES_STRUCTURED / f\"plate-{str(plate_number).zfill(3)}\"\n",
    "\n",
    "def load_plate_manifest(plate_number: int) -> dict:\n",
    "    plate_dir = get_plate_dir(plate_number)\n",
    "    return json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Project loaded successfully.\")\n",
    "print(f\"Dataset root      : {DATASET_ROOT}\")\n",
    "print(f\"Structured plates : {len(plate_dirs)}\")\n",
    "print(\"Ready for runs, embeddings, or analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure & naming contract assertion (read-only)\n",
    "\n",
    "Purpose:\n",
    "- Re-assert filesystem law\n",
    "- Confirm naming conventions\n",
    "- Abort early if drift has occurred\n",
    "\n",
    "This cell reads only and raises on any deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"STRUCTURE & NAMING ASSERTION\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "assert DATASET_ROOT.exists(), \"DATASET_ROOT missing\"\n",
    "assert PLATES_STRUCTURED.exists(), \"plates_structured missing\"\n",
    "assert SCHEMA_DIR.exists(), \"schemas missing\"\n",
    "assert LEDGER_DIR.exists(), \"ledger missing\"\n",
    "\n",
    "print(\"[1] Canonical roots present (OK)\")\n",
    "\n",
    "plate_dirs = sorted(p for p in PLATES_STRUCTURED.iterdir() if p.is_dir())\n",
    "assert len(plate_dirs) == 435, \"Plate count must be exactly 435\"\n",
    "\n",
    "plate_name_re = re.compile(r\"^plate-\\d{3}$\")\n",
    "bad_names = [p.name for p in plate_dirs if not plate_name_re.match(p.name)]\n",
    "assert not bad_names, f\"Invalid plate directory names: {bad_names[:5]}\"\n",
    "\n",
    "print(\"[2] Plate directory naming OK (OK)\")\n",
    "\n",
    "required_subdirs = {\"source\", \"runs\", \"viz\", \"cache\"}\n",
    "violations = []\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    contents = {p.name for p in plate_dir.iterdir()}\n",
    "    missing = required_subdirs - contents\n",
    "    if missing:\n",
    "        violations.append(f\"{plate_dir.name}: missing {missing}\")\n",
    "\n",
    "    source_dir = plate_dir / \"source\"\n",
    "    if source_dir.exists():\n",
    "        files = [p for p in source_dir.iterdir() if p.is_file()]\n",
    "        if len(files) != 1:\n",
    "            violations.append(f\"{plate_dir.name}: source/ has {len(files)} files\")\n",
    "\n",
    "    for fname in (\"manifest.json\", \"source.sha256\"):\n",
    "        if not (plate_dir / fname).exists():\n",
    "            violations.append(f\"{plate_dir.name}: missing {fname}\")\n",
    "\n",
    "assert not violations, f\"Plate structure violations:\\n{violations[:5]}\"\n",
    "\n",
    "print(\"[3] Plate internal structure OK (OK)\")\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    manifest = json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    assert manifest[\"plate_id\"] == plate_dir.name, f\"{plate_dir.name}: plate_id mismatch\"\n",
    "    src = plate_dir / manifest[\"source_image\"]\n",
    "    assert src.exists(), f\"{plate_dir.name}: source_image missing\"\n",
    "\n",
    "print(\"[4] Manifest <-> filesystem consistency OK (OK)\")\n",
    "\n",
    "run_re = re.compile(r\"^run-\\d{8}-\\d{6}-[a-f0-9]{8}$\")\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    runs_dir = plate_dir / \"runs\"\n",
    "    if not runs_dir.exists():\n",
    "        continue\n",
    "    for run in runs_dir.iterdir():\n",
    "        if not run.is_dir():\n",
    "            continue\n",
    "        assert run_re.match(run.name), f\"{plate_dir.name}: invalid run dir {run.name}\"\n",
    "        assert (run / \"metrics.json\").exists(), f\"{run}: missing metrics.json\"\n",
    "\n",
    "print(\"[5] Run directory naming (if any) OK (OK)\")\n",
    "\n",
    "expected_ledgers = {\n",
    "    \"plates.parquet\",\n",
    "    \"runs.parquet\",\n",
    "    \"embeddings.parquet\",\n",
    "    \"segments.parquet\",\n",
    "}\n",
    "\n",
    "found_ledgers = {p.name for p in LEDGER_DIR.iterdir() if p.is_file()}\n",
    "assert expected_ledgers.issubset(found_ledgers), (\n",
    "    f\"Ledger files missing: {expected_ledgers - found_ledgers}\"\n",
    ")\n",
    "\n",
    "print(\"[6] Ledger scaffolding present (OK)\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"CONTRACT STATUS: ASSERTED\")\n",
    "print(\"==============================\")\n",
    "print(\n",
    "    \"\\n\"\n",
    "    \"Filesystem, naming, and structural invariants\\n\"\n",
    "    \"are intact.\\n\\n\"\n",
    "    \"It is now safe to:\\n\"\n",
    "    \"- start new runs\\n\"\n",
    "    \"- generate embeddings\\n\"\n",
    "    \"- perform segmentation\\n\"\n",
    "    \"- write derived artifacts\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies (Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow numpy matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: remove baseline image metadata outputs\n",
    "\n",
    "Deletes only `input_image.json` under each `plates_structured/<plate_id>/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "plates_structured = DATASET_ROOT / \"plates_structured\"\n",
    "\n",
    "removed = 0\n",
    "\n",
    "for plate_dir in plates_structured.iterdir():\n",
    "    if not plate_dir.is_dir():\n",
    "        continue\n",
    "    target = plate_dir / \"input_image.json\"\n",
    "    if target.exists():\n",
    "        target.unlink()\n",
    "        removed += 1\n",
    "\n",
    "print(\"Cleanup complete.\")\n",
    "print(f\"Removed input_image.json files: {removed}\")\n",
    "print(\"Dataset restored to pre-baseline state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo-wide exploratory (read-only, CPU-only)\n",
    "\n",
    "Header-only inspection of source images; no writes and no full pixel loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "ROOT = DATASET_ROOT\n",
    "PLATES_STRUCTURED = ROOT / \"plates_structured\"\n",
    "LEDGER_DIR = ROOT / \"ledger\"\n",
    "SCHEMA_DIR = ROOT / \"schemas\"\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"REPO EXPLORATORY REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "top_dirs = sorted([p.name for p in ROOT.iterdir() if p.is_dir()])\n",
    "top_files = sorted([p.name for p in ROOT.iterdir() if p.is_file()])\n",
    "\n",
    "print(\"[1] Top-level directories:\")\n",
    "for d in top_dirs:\n",
    "    print(\"  -\", d)\n",
    "\n",
    "print(\"\\n[1] Top-level files:\")\n",
    "for f in top_files:\n",
    "    print(\"  -\", f)\n",
    "\n",
    "plate_dirs = sorted(\n",
    "    p for p in PLATES_STRUCTURED.iterdir()\n",
    "    if p.is_dir() and p.name.startswith(\"plate-\")\n",
    ")\n",
    "\n",
    "print(f\"\\n[2] Plates found: {len(plate_dirs)}\")\n",
    "\n",
    "missing_manifest = []\n",
    "missing_source = []\n",
    "run_counts = []\n",
    "\n",
    "for p in plate_dirs:\n",
    "    if not (p / \"manifest.json\").exists():\n",
    "        missing_manifest.append(p.name)\n",
    "\n",
    "    src_dir = p / \"source\"\n",
    "    if not src_dir.exists():\n",
    "        missing_source.append(p.name)\n",
    "    else:\n",
    "        files = list(src_dir.iterdir())\n",
    "        if len(files) != 1:\n",
    "            missing_source.append(p.name)\n",
    "\n",
    "    runs_dir = p / \"runs\"\n",
    "    if runs_dir.exists():\n",
    "        run_counts.append(len([r for r in runs_dir.iterdir() if r.is_dir()]))\n",
    "    else:\n",
    "        run_counts.append(0)\n",
    "\n",
    "print(\"    Missing manifest.json:\", len(missing_manifest))\n",
    "print(\"    Missing/invalid source:\", len(missing_source))\n",
    "print(\"    Plates with runs:\", sum(1 for c in run_counts if c > 0))\n",
    "print(\"    Total runs:\", sum(run_counts))\n",
    "\n",
    "print(\"\\n[3] Manifest field coverage (sampled)\")\n",
    "\n",
    "key_counter = Counter()\n",
    "sampled = 0\n",
    "\n",
    "for p in plate_dirs[:50]:\n",
    "    try:\n",
    "        m = json.loads((p / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "        key_counter.update(m.keys())\n",
    "        sampled += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for k, v in key_counter.most_common():\n",
    "    print(f\"  {k:20s} -> present in {v}/{sampled}\")\n",
    "\n",
    "print(\"\\n[4] Image header stats (source images only)\")\n",
    "\n",
    "sizes = []\n",
    "huge = []\n",
    "\n",
    "for p in plate_dirs:\n",
    "    try:\n",
    "        src = next((p / \"source\").iterdir())\n",
    "        with Image.open(src) as img:\n",
    "            w, h = img.size\n",
    "        mp = (w * h) / 1_000_000\n",
    "        sizes.append(mp)\n",
    "        if mp > 90:\n",
    "            huge.append((p.name, round(mp, 2)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if sizes:\n",
    "    print(f\"    Min megapixels: {round(min(sizes), 2)}\")\n",
    "    print(f\"    Max megapixels: {round(max(sizes), 2)}\")\n",
    "    print(f\"    Mean megapixels: {round(sum(sizes)/len(sizes), 2)}\")\n",
    "    print(f\"    Images > 90 MP (PIL warning risk): {len(huge)}\")\n",
    "\n",
    "for name, mp in huge[:5]:\n",
    "    print(f\"      - {name}: {mp} MP\")\n",
    "\n",
    "print(\"\\n[5] Ledger files\")\n",
    "if LEDGER_DIR.exists():\n",
    "    for p in sorted(LEDGER_DIR.iterdir()):\n",
    "        if p.is_file():\n",
    "            print(f\"  - {p.name:20s} {round(p.stat().st_size/1024, 1)} KB\")\n",
    "else:\n",
    "    print(\"  Ledger directory missing\")\n",
    "\n",
    "print(\"\\n[5] Schemas\")\n",
    "if SCHEMA_DIR.exists():\n",
    "    for p in sorted(SCHEMA_DIR.iterdir()):\n",
    "        print(\"  -\", p.name)\n",
    "else:\n",
    "    print(\"  Schema directory missing\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"==============================\")\n",
    "print(\n",
    "    f\"\\n\"\n",
    "    f\"Plates                : {len(plate_dirs)}\\n\"\n",
    "    f\"Total runs            : {sum(run_counts)}\\n\"\n",
    "    f\"Images > 90 MP        : {len(huge)}\\n\"\n",
    "    f\"Manifests missing     : {len(missing_manifest)}\\n\"\n",
    "    f\"Source issues         : {len(missing_source)}\\n\"\n",
    "    f\"Schemas present       : {SCHEMA_DIR.exists()}\\n\"\n",
    "    f\"Ledgers present       : {LEDGER_DIR.exists()}\\n\"\n",
    ")\n",
    "\n",
    "print(\"Exploratory complete. No files written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Baseline (SageMaker-style: write-only output root)\n",
    "\n",
    "This section is designed to run in Google Colab Pro (CPU) while matching the SageMaker mental model: treat the dataset as read-only input and write all new artifacts to a separate output root.\n",
    "\n",
    "### Input criteria\n",
    "\n",
    "- `INPUT_ROOT` points at a scaffolded dataset root containing `plates_structured/`, `schemas/`, and `data.json`.\n",
    "- `plates_structured/` contains 435 `plate-###/` directories.\n",
    "- Each `plate-###/` contains `manifest.json`, `source.sha256`, and `source/plate-###.jpg`.\n",
    "- `schemas/plate.manifest.schema.json` and `schemas/run.manifest.schema.json` exist and are draft-2020-12 valid.\n",
    "\n",
    "### Midpoint artifacts\n",
    "\n",
    "- `OUTPUT_ROOT/schemas/cpu.baseline.schema.json` is written once if missing.\n",
    "- `OUTPUT_ROOT/reports/<run_id>/report.json` summarizes shard parameters, counts, and failures.\n",
    "\n",
    "### Outputs (append-only, per plate)\n",
    "\n",
    "For each plate in the shard, the job writes:\n",
    "\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/metrics.json`\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/cpu_baseline.json`\n",
    "\n",
    "### Constraints enforced\n",
    "\n",
    "- The input dataset is never mutated.\n",
    "- Outputs are run-scoped and append-only; no overwrites of prior runs.\n",
    "- Each `cpu_baseline.json` validates against `cpu.baseline.schema.json` (when present).\n",
    "- Each `metrics.json` validates against `run.manifest.schema.json`.\n",
    "\n",
    "### CPU-only feature pack\n",
    "\n",
    "- Container facts: bytes, extension, format, sha256, EXIF flag, ICC flag+hash, JPEG progressive/subsampling/quant hash where exposed\n",
    "- Geometry: width, height, megapixels, aspect ratio, tiling lattice\n",
    "- Pixel distribution: RGB histograms and luma histogram (256 bins)\n",
    "- Summary stats: mean/std/min/max + clipped ratios per channel + luma\n",
    "- Entropy per channel + luma\n",
    "- Hashes: aHash, dHash, pHash\n",
    "- Sharpness proxy: Laplacian variance on downsampled grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from jsonschema import Draft202012Validator\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "TILE_SIZE = 512\n",
    "HIST_BINS = 256\n",
    "LAPLACE_MAX_DIM = 1024\n",
    "PHASH_BITS = 8\n",
    "PHASH_FACTOR = 4\n",
    "\n",
    "def utc_iso():\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def generate_run_id(models: list[str], note: str) -> str:\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    payload = \"|\".join(models) + \"|\" + note\n",
    "    short = hashlib.sha1(payload.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"run-{stamp}-{short}\"\n",
    "\n",
    "def resize_fit(img: Image.Image, max_dim: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    m = max(w, h)\n",
    "    if m <= max_dim:\n",
    "        return img\n",
    "    s = max_dim / m\n",
    "    return img.resize((max(1, int(round(w * s))), max(1, int(round(h * s)))), resample=Image.BILINEAR)\n",
    "\n",
    "def laplacian_variance(gray_img: Image.Image) -> float:\n",
    "    g = resize_fit(gray_img, LAPLACE_MAX_DIM)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    ap = np.pad(a, ((1, 1), (1, 1)), mode=\"edge\")\n",
    "    out = ap[:-2, 1:-1] + ap[2:, 1:-1] + ap[1:-1, :-2] + ap[1:-1, 2:] - 4.0 * ap[1:-1, 1:-1]\n",
    "    return float(np.var(out))\n",
    "\n",
    "def hist_stats(hist: list[int]) -> dict:\n",
    "    total = int(sum(hist))\n",
    "    if total <= 0:\n",
    "        return {\"total\": 0, \"min\": None, \"max\": None, \"mean\": None, \"std\": None, \"clipped_low_ratio\": None, \"clipped_high_ratio\": None}\n",
    "    mn = next((i for i, c in enumerate(hist) if c), 0)\n",
    "    mx = next((255 - i for i, c in enumerate(reversed(hist)) if c), 255)\n",
    "    mean = sum(i * c for i, c in enumerate(hist)) / total\n",
    "    s2 = sum((i - mean) ** 2 * c for i, c in enumerate(hist)) / total\n",
    "    return {\"total\": total, \"min\": int(mn), \"max\": int(mx), \"mean\": float(mean), \"std\": float(math.sqrt(s2)), \"clipped_low_ratio\": float(hist[0] / total), \"clipped_high_ratio\": float(hist[-1] / total)}\n",
    "\n",
    "def entropy(hist: list[int]) -> float | None:\n",
    "    total = float(sum(hist))\n",
    "    if total <= 0:\n",
    "        return None\n",
    "    ent = 0.0\n",
    "    for c in hist:\n",
    "        if c:\n",
    "            p = c / total\n",
    "            ent -= p * math.log2(p)\n",
    "    return float(ent)\n",
    "\n",
    "def ahash(img: Image.Image, size: int = 8) -> str:\n",
    "    g = img.convert(\"L\").resize((size, size), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    m = a.mean()\n",
    "    bits = (a > m).astype(np.uint8).flatten()\n",
    "    v = 0\n",
    "    for b in bits:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(size * size) // 4}x}\"\n",
    "\n",
    "def dhash(img: Image.Image, size: int = 8) -> str:\n",
    "    g = img.convert(\"L\").resize((size + 1, size), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    diff = (a[:, 1:] > a[:, :-1]).astype(np.uint8).flatten()\n",
    "    v = 0\n",
    "    for b in diff:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(size * size) // 4}x}\"\n",
    "\n",
    "def phash(img: Image.Image, hash_bits: int = PHASH_BITS, highfreq_factor: int = PHASH_FACTOR) -> str:\n",
    "    n = hash_bits * highfreq_factor\n",
    "    g = img.convert(\"L\").resize((n, n), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    idx = np.arange(n)\n",
    "    k = idx.reshape(-1, 1)\n",
    "    cos = np.cos(np.pi / n * (idx + 0.5) * k).astype(np.float32)\n",
    "    d1 = cos @ a\n",
    "    d2 = (cos @ d1.T).T\n",
    "    d = d2[:hash_bits, :hash_bits].flatten()\n",
    "    med = np.median(d[1:]) if d.size > 1 else np.median(d)\n",
    "    bits = (d > med).astype(np.uint8)\n",
    "    v = 0\n",
    "    for b in bits:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(hash_bits * hash_bits) // 4}x}\"\n",
    "\n",
    "def exif_present(img) -> bool:\n",
    "    try:\n",
    "        ex = img.getexif()\n",
    "        return ex is not None and len(ex) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def icc_hash(img):\n",
    "    try:\n",
    "        icc = img.info.get(\"icc_profile\", None)\n",
    "        if not icc:\n",
    "            return False, None\n",
    "        if isinstance(icc, str):\n",
    "            icc = icc.encode(\"utf-8\", errors=\"ignore\")\n",
    "        return True, hashlib.sha256(icc).hexdigest()\n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "def jpeg_progressive_flag(img):\n",
    "    v = img.info.get(\"progressive\", None)\n",
    "    return None if v is None else bool(v)\n",
    "\n",
    "def jpeg_subsampling_flag(img):\n",
    "    v = img.info.get(\"subsampling\", None)\n",
    "    return None if v is None else str(v)\n",
    "\n",
    "def quant_hash_from_pil(img):\n",
    "    q = getattr(img, \"quantization\", None)\n",
    "    if not q:\n",
    "        return None\n",
    "    try:\n",
    "        payload = json.dumps(q, sort_keys=True)\n",
    "        return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_cpu_schema_if_missing(output_root: Path):\n",
    "    schema_dir = output_root / \"schemas\"\n",
    "    schema_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cpu_path = schema_dir / \"cpu.baseline.schema.json\"\n",
    "    if cpu_path.exists():\n",
    "        return cpu_path\n",
    "    schema = {\n",
    "        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "        \"$id\": \"https://burning-world-series/schemas/cpu.baseline.schema.json\",\n",
    "        \"title\": \"CPU Baseline Feature Pack\",\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"plate_id\", \"run_id\", \"timestamp\", \"source_image\", \"source_file\", \"geometry\", \"tiling\", \"decode\"],\n",
    "        \"properties\": {\n",
    "            \"plate_id\": {\"type\": \"string\", \"pattern\": \"^plate-[0-9]{3}$\"},\n",
    "            \"run_id\": {\"type\": \"string\"},\n",
    "            \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "            \"source_image\": {\"type\": \"string\"},\n",
    "            \"source_file\": {\"type\": \"object\"},\n",
    "            \"geometry\": {\"type\": \"object\"},\n",
    "            \"tiling\": {\"type\": \"object\"},\n",
    "            \"decode\": {\"type\": \"object\"},\n",
    "            \"pixel_stats\": {\"type\": [\"object\", \"null\"]},\n",
    "            \"hashes\": {\"type\": [\"object\", \"null\"]},\n",
    "        },\n",
    "        \"additionalProperties\": False,\n",
    "    }\n",
    "    cpu_path.write_text(json.dumps(schema, indent=2), encoding=\"utf-8\")\n",
    "    Draft202012Validator.check_schema(schema)\n",
    "    return cpu_path\n",
    "\n",
    "def load_validator(path: Path) -> Draft202012Validator:\n",
    "    schema = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    Draft202012Validator.check_schema(schema)\n",
    "    return Draft202012Validator(schema)\n",
    "\n",
    "def cpu_baseline_job(input_root: Path, output_root: Path, shard_index: int, shard_count: int, run_id: str | None, skip_if_present: bool):\n",
    "    plates_root = input_root / \"plates_structured\"\n",
    "    schemas_root = input_root / \"schemas\"\n",
    "    if not plates_root.exists():\n",
    "        raise RuntimeError(f\"Missing plates_structured: {plates_root}\")\n",
    "    if not schemas_root.exists():\n",
    "        raise RuntimeError(f\"Missing schemas: {schemas_root}\")\n",
    "\n",
    "    plate_validator = load_validator(schemas_root / \"plate.manifest.schema.json\")\n",
    "    run_validator = load_validator(schemas_root / \"run.manifest.schema.json\")\n",
    "\n",
    "    cpu_schema_path = schemas_root / \"cpu.baseline.schema.json\"\n",
    "    if cpu_schema_path.exists():\n",
    "        cpu_validator = load_validator(cpu_schema_path)\n",
    "    else:\n",
    "        cpu_validator = load_validator(write_cpu_schema_if_missing(output_root))\n",
    "\n",
    "    plates = sorted([p for p in plates_root.iterdir() if p.is_dir() and p.name.startswith(\"plate-\")])\n",
    "    if len(plates) != 435:\n",
    "        raise RuntimeError(f\"Unexpected plate count: {len(plates)}\")\n",
    "\n",
    "    selected = [p for i, p in enumerate(plates) if i % shard_count == shard_index]\n",
    "    models = [\"cpu-baseline-v1\"]\n",
    "    note = \"cpu baseline: container geometry hist entropy hashes laplacian\"\n",
    "    rid = run_id or generate_run_id(models, note)\n",
    "\n",
    "    report = {\n",
    "        \"run_id\": rid,\n",
    "        \"timestamp\": utc_iso(),\n",
    "        \"input_root\": str(input_root),\n",
    "        \"output_root\": str(output_root),\n",
    "        \"shard_index\": shard_index,\n",
    "        \"shard_count\": shard_count,\n",
    "        \"plates_total\": len(plates),\n",
    "        \"plates_selected\": len(selected),\n",
    "        \"plates_processed\": 0,\n",
    "        \"plates_skipped\": 0,\n",
    "        \"decode_failures\": 0,\n",
    "        \"schema_failures\": 0,\n",
    "        \"errors_sample\": [],\n",
    "    }\n",
    "\n",
    "    for plate_dir in tqdm(selected, desc=\"plates\"):\n",
    "        manifest = json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "        if list(plate_validator.iter_errors(manifest)):\n",
    "            report[\"schema_failures\"] += 1\n",
    "            if len(report[\"errors_sample\"]) < 10:\n",
    "                report[\"errors_sample\"].append(f\"{plate_dir.name}: plate manifest schema failure\")\n",
    "            continue\n",
    "\n",
    "        src_path = plate_dir / manifest[\"source_image\"]\n",
    "        if not src_path.exists():\n",
    "            report[\"decode_failures\"] += 1\n",
    "            if len(report[\"errors_sample\"]) < 10:\n",
    "                report[\"errors_sample\"].append(f\"{plate_dir.name}: missing source image\")\n",
    "            continue\n",
    "\n",
    "        out_plate_dir = output_root / \"plates_structured\" / plate_dir.name\n",
    "        out_run_dir = out_plate_dir / \"runs\" / rid\n",
    "        out_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        out_cpu = out_run_dir / \"cpu_baseline.json\"\n",
    "        out_metrics = out_run_dir / \"metrics.json\"\n",
    "\n",
    "        if skip_if_present and out_cpu.exists() and out_metrics.exists():\n",
    "            report[\"plates_skipped\"] += 1\n",
    "            continue\n",
    "\n",
    "        run_manifest = {\n",
    "            \"run_id\": rid,\n",
    "            \"plate_id\": plate_dir.name,\n",
    "            \"timestamp\": utc_iso(),\n",
    "            \"models\": models,\n",
    "            \"outputs\": [\"cpu_baseline.json\"],\n",
    "            \"notes\": note,\n",
    "        }\n",
    "\n",
    "        if list(run_validator.iter_errors(run_manifest)):\n",
    "            report[\"schema_failures\"] += 1\n",
    "            if len(report[\"errors_sample\"]) < 10:\n",
    "                report[\"errors_sample\"].append(f\"{plate_dir.name}: run manifest schema failure\")\n",
    "            continue\n",
    "\n",
    "        baseline = {\n",
    "            \"plate_id\": manifest[\"plate_id\"],\n",
    "            \"run_id\": rid,\n",
    "            \"timestamp\": utc_iso(),\n",
    "            \"source_image\": manifest[\"source_image\"],\n",
    "            \"source_file\": {\n",
    "                \"bytes\": int(src_path.stat().st_size),\n",
    "                \"extension\": src_path.suffix.lower().lstrip(\".\"),\n",
    "                \"format\": None,\n",
    "                \"sha256\": sha256_file(src_path),\n",
    "                \"exif_present\": False,\n",
    "                \"icc_present\": False,\n",
    "                \"icc_hash\": None,\n",
    "                \"jpeg_is_progressive\": None,\n",
    "                \"jpeg_subsampling\": None,\n",
    "                \"jpeg_quant_hash\": None,\n",
    "            },\n",
    "            \"geometry\": {\"width_px\": None, \"height_px\": None, \"megapixels\": None, \"aspect_ratio\": None, \"mode\": None},\n",
    "            \"tiling\": {\"tile_size_px\": TILE_SIZE, \"tiles_x\": 1, \"tiles_y\": 1, \"total_tiles\": 1},\n",
    "            \"decode\": {\"ok\": False, \"error\": None},\n",
    "            \"pixel_stats\": None,\n",
    "            \"hashes\": None,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with Image.open(src_path) as img:\n",
    "                baseline[\"source_file\"][\"format\"] = img.format\n",
    "                baseline[\"source_file\"][\"exif_present\"] = exif_present(img)\n",
    "                ip, ih = icc_hash(img)\n",
    "                baseline[\"source_file\"][\"icc_present\"] = ip\n",
    "                baseline[\"source_file\"][\"icc_hash\"] = ih\n",
    "                if (img.format or \"\").upper() == \"JPEG\":\n",
    "                    baseline[\"source_file\"][\"jpeg_is_progressive\"] = jpeg_progressive_flag(img)\n",
    "                    baseline[\"source_file\"][\"jpeg_subsampling\"] = jpeg_subsampling_flag(img)\n",
    "                    baseline[\"source_file\"][\"jpeg_quant_hash\"] = quant_hash_from_pil(img)\n",
    "\n",
    "                w, h = img.size\n",
    "                baseline[\"geometry\"] = {\"width_px\": int(w), \"height_px\": int(h), \"megapixels\": float(round((w * h) / 1_000_000, 3)), \"aspect_ratio\": float(w / h), \"mode\": img.mode}\n",
    "                tiles_x = (w + TILE_SIZE - 1) // TILE_SIZE\n",
    "                tiles_y = (h + TILE_SIZE - 1) // TILE_SIZE\n",
    "                baseline[\"tiling\"] = {\"tile_size_px\": TILE_SIZE, \"tiles_x\": int(tiles_x), \"tiles_y\": int(tiles_y), \"total_tiles\": int(tiles_x * tiles_y)}\n",
    "\n",
    "                rgb = img.convert(\"RGB\")\n",
    "                raw = rgb.histogram()\n",
    "                Rh, Gh, Bh = raw[0:256], raw[256:512], raw[512:768]\n",
    "                L = rgb.convert(\"L\")\n",
    "                Lh = L.histogram()\n",
    "\n",
    "                baseline[\"pixel_stats\"] = {\n",
    "                    \"colorspace\": \"as-decoded\",\n",
    "                    \"rgb_histograms\": {\"bins\": HIST_BINS, \"R\": Rh, \"G\": Gh, \"B\": Bh},\n",
    "                    \"l_histogram\": {\"bins\": HIST_BINS, \"L\": Lh},\n",
    "                    \"rgb_stats\": {\"R\": hist_stats(Rh), \"G\": hist_stats(Gh), \"B\": hist_stats(Bh)},\n",
    "                    \"luma_stats\": hist_stats(Lh),\n",
    "                    \"entropy\": {\"R\": entropy(Rh), \"G\": entropy(Gh), \"B\": entropy(Bh), \"L\": entropy(Lh)},\n",
    "                    \"laplacian_var\": laplacian_variance(L),\n",
    "                }\n",
    "                baseline[\"hashes\"] = {\"ahash\": ahash(rgb), \"dhash\": dhash(rgb), \"phash\": phash(rgb)}\n",
    "                baseline[\"decode\"] = {\"ok\": True, \"error\": None}\n",
    "\n",
    "        except Exception as e:\n",
    "            report[\"decode_failures\"] += 1\n",
    "            baseline[\"decode\"] = {\"ok\": False, \"error\": f\"{type(e).__name__}: {str(e)[:300]}\"}\n",
    "\n",
    "        errs = list(cpu_validator.iter_errors(baseline))\n",
    "        if errs:\n",
    "            report[\"schema_failures\"] += 1\n",
    "            if len(report[\"errors_sample\"]) < 10:\n",
    "                report[\"errors_sample\"].append(f\"{plate_dir.name}: cpu baseline schema failure\")\n",
    "            continue\n",
    "\n",
    "        out_metrics.write_text(json.dumps(run_manifest, indent=2), encoding=\"utf-8\")\n",
    "        out_cpu.write_text(json.dumps(baseline, indent=2), encoding=\"utf-8\")\n",
    "        report[\"plates_processed\"] += 1\n",
    "\n",
    "    report_dir = output_root / \"reports\" / rid\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (report_dir / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "    return report\n",
    "\n",
    "INPUT_ROOT = Path(os.environ.get(\"BWS_INPUT_ROOT\", str(DATASET_ROOT))).expanduser().resolve()\n",
    "OUTPUT_ROOT = Path(os.environ.get(\"BWS_OUTPUT_ROOT\", str(DATASET_ROOT / \"_RUN_OUTPUT\"))).expanduser().resolve()\n",
    "SHARD_INDEX = int(os.environ.get(\"BWS_SHARD_INDEX\", \"0\"))\n",
    "SHARD_COUNT = int(os.environ.get(\"BWS_SHARD_COUNT\", \"1\"))\n",
    "RUN_ID = os.environ.get(\"BWS_RUN_ID\", None)\n",
    "SKIP_IF_PRESENT = os.environ.get(\"BWS_SKIP_IF_PRESENT\", \"1\") == \"1\"\n",
    "\n",
    "print(\"INPUT_ROOT:\", INPUT_ROOT)\n",
    "print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n",
    "print(\"SHARD_INDEX/COUNT:\", SHARD_INDEX, SHARD_COUNT)\n",
    "\n",
    "report = cpu_baseline_job(\n",
    "    input_root=INPUT_ROOT,\n",
    "    output_root=OUTPUT_ROOT,\n",
    "    shard_index=SHARD_INDEX,\n",
    "    shard_count=SHARD_COUNT,\n",
    "    run_id=RUN_ID,\n",
    "    skip_if_present=SKIP_IF_PRESENT,\n",
    ")\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
