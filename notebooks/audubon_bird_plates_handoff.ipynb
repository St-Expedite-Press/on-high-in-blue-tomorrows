{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b0ff19",
   "metadata": {},
   "source": [
    "# Audubon Bird Plates - Handoff + Contract Assertions\n",
    "\n",
    "Use this notebook as the entrypoint once a dataset has been scaffolded (i.e., `plates_structured/`, schemas, checksums, ledger).\n",
    "\n",
    "Dataset root resolution:\n",
    "- Colab: mounts Drive and searches under `/content/drive/MyDrive/burning-world-series/`\n",
    "- Local: searches upward from the current working directory for a folder starting with `audubon-bird-plates`\n",
    "- Override: set `BWS_DATASET_ROOT` to an explicit path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b6eb6",
   "metadata": {},
   "source": [
    "## Handoff: load project context (read-only)\n",
    "\n",
    "Purpose:\n",
    "- Resolve dataset root\n",
    "- Expose canonical paths\n",
    "- Perform light sanity checks only\n",
    "\n",
    "This cell must:\n",
    "- Not write files\n",
    "- Not validate schemas deeply\n",
    "- Not compute checksums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_dataset_root(start: Path) -> Path:\n",
    "    override = os.environ.get(\"BWS_DATASET_ROOT\")\n",
    "    if override:\n",
    "        p = Path(override).expanduser()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise RuntimeError(f\"BWS_DATASET_ROOT does not exist: {p}\")\n",
    "\n",
    "    def is_dataset_dir(p: Path) -> bool:\n",
    "        if not p.is_dir():\n",
    "            return False\n",
    "        if not p.name.startswith(\"audubon-bird-plates\"):\n",
    "            return False\n",
    "        if not (p / \"data.json\").exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    if is_dataset_dir(start):\n",
    "        return start\n",
    "\n",
    "    for base in [start] + list(start.parents):\n",
    "        try:\n",
    "            children = list(base.iterdir())\n",
    "        except Exception:\n",
    "            continue\n",
    "        for child in sorted(children):\n",
    "            if is_dataset_dir(child):\n",
    "                return child\n",
    "\n",
    "    raise RuntimeError(\"Could not find dataset root; set BWS_DATASET_ROOT\")\n",
    "\n",
    "if in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    project_root = Path(\"/content/drive/MyDrive/burning-world-series\")\n",
    "    assert project_root.exists(), \"Project root missing\"\n",
    "    DATASET_ROOT = find_dataset_root(project_root)\n",
    "else:\n",
    "    DATASET_ROOT = find_dataset_root(Path.cwd())\n",
    "\n",
    "PLATES_ORIGINAL = DATASET_ROOT / \"plates\"\n",
    "PLATES_STRUCTURED = DATASET_ROOT / \"plates_structured\"\n",
    "SCHEMA_DIR = DATASET_ROOT / \"schemas\"\n",
    "LEDGER_DIR = DATASET_ROOT / \"ledger\"\n",
    "DATA_JSON = DATASET_ROOT / \"data.json\"\n",
    "README_MD = DATASET_ROOT / \"README.md\"\n",
    "\n",
    "assert PLATES_STRUCTURED.exists(), \"plates_structured missing (run audubon_bird_plates_setup.ipynb first)\"\n",
    "assert SCHEMA_DIR.exists(), \"schemas missing\"\n",
    "assert LEDGER_DIR.exists(), \"ledger missing\"\n",
    "assert DATA_JSON.exists(), \"data.json missing\"\n",
    "\n",
    "plate_dirs = [p for p in PLATES_STRUCTURED.iterdir() if p.is_dir()]\n",
    "assert len(plate_dirs) == 435, \"Unexpected plate count\"\n",
    "\n",
    "def get_plate_dir(plate_number: int) -> Path:\n",
    "    return PLATES_STRUCTURED / f\"plate-{str(plate_number).zfill(3)}\"\n",
    "\n",
    "def load_plate_manifest(plate_number: int) -> dict:\n",
    "    plate_dir = get_plate_dir(plate_number)\n",
    "    return json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Project loaded successfully.\")\n",
    "print(f\"Dataset root      : {DATASET_ROOT}\")\n",
    "print(f\"Structured plates : {len(plate_dirs)}\")\n",
    "print(\"Ready for runs, embeddings, or analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure & naming contract assertion (read-only)\n",
    "\n",
    "Purpose:\n",
    "- Re-assert filesystem law\n",
    "- Confirm naming conventions\n",
    "- Abort early if drift has occurred\n",
    "\n",
    "This cell reads only and raises on any deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"STRUCTURE & NAMING ASSERTION\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "assert DATASET_ROOT.exists(), \"DATASET_ROOT missing\"\n",
    "assert PLATES_STRUCTURED.exists(), \"plates_structured missing\"\n",
    "assert SCHEMA_DIR.exists(), \"schemas missing\"\n",
    "assert LEDGER_DIR.exists(), \"ledger missing\"\n",
    "\n",
    "print(\"[1] Canonical roots present (OK)\")\n",
    "\n",
    "plate_dirs = sorted(p for p in PLATES_STRUCTURED.iterdir() if p.is_dir())\n",
    "assert len(plate_dirs) == 435, \"Plate count must be exactly 435\"\n",
    "\n",
    "plate_name_re = re.compile(r\"^plate-\\d{3}$\")\n",
    "bad_names = [p.name for p in plate_dirs if not plate_name_re.match(p.name)]\n",
    "assert not bad_names, f\"Invalid plate directory names: {bad_names[:5]}\"\n",
    "\n",
    "print(\"[2] Plate directory naming OK (OK)\")\n",
    "\n",
    "required_subdirs = {\"source\", \"runs\", \"viz\", \"cache\"}\n",
    "violations = []\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    contents = {p.name for p in plate_dir.iterdir()}\n",
    "    missing = required_subdirs - contents\n",
    "    if missing:\n",
    "        violations.append(f\"{plate_dir.name}: missing {missing}\")\n",
    "\n",
    "    source_dir = plate_dir / \"source\"\n",
    "    if source_dir.exists():\n",
    "        files = [p for p in source_dir.iterdir() if p.is_file()]\n",
    "        if len(files) != 1:\n",
    "            violations.append(f\"{plate_dir.name}: source/ has {len(files)} files\")\n",
    "\n",
    "    for fname in (\"manifest.json\", \"source.sha256\"):\n",
    "        if not (plate_dir / fname).exists():\n",
    "            violations.append(f\"{plate_dir.name}: missing {fname}\")\n",
    "\n",
    "assert not violations, f\"Plate structure violations:\\n{violations[:5]}\"\n",
    "\n",
    "print(\"[3] Plate internal structure OK (OK)\")\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    manifest = json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    assert manifest[\"plate_id\"] == plate_dir.name, f\"{plate_dir.name}: plate_id mismatch\"\n",
    "    src = plate_dir / manifest[\"source_image\"]\n",
    "    assert src.exists(), f\"{plate_dir.name}: source_image missing\"\n",
    "\n",
    "print(\"[4] Manifest <-> filesystem consistency OK (OK)\")\n",
    "\n",
    "run_re = re.compile(r\"^run-\\d{8}-\\d{6}-[a-f0-9]{8}$\")\n",
    "\n",
    "for plate_dir in plate_dirs:\n",
    "    runs_dir = plate_dir / \"runs\"\n",
    "    if not runs_dir.exists():\n",
    "        continue\n",
    "    for run in runs_dir.iterdir():\n",
    "        if not run.is_dir():\n",
    "            continue\n",
    "        assert run_re.match(run.name), f\"{plate_dir.name}: invalid run dir {run.name}\"\n",
    "        assert (run / \"metrics.json\").exists(), f\"{run}: missing metrics.json\"\n",
    "\n",
    "print(\"[5] Run directory naming (if any) OK (OK)\")\n",
    "\n",
    "expected_ledgers = {\n",
    "    \"plates.parquet\",\n",
    "    \"runs.parquet\",\n",
    "    \"embeddings.parquet\",\n",
    "    \"segments.parquet\",\n",
    "}\n",
    "\n",
    "found_ledgers = {p.name for p in LEDGER_DIR.iterdir() if p.is_file()}\n",
    "assert expected_ledgers.issubset(found_ledgers), (\n",
    "    f\"Ledger files missing: {expected_ledgers - found_ledgers}\"\n",
    ")\n",
    "\n",
    "print(\"[6] Ledger scaffolding present (OK)\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"CONTRACT STATUS: ASSERTED\")\n",
    "print(\"==============================\")\n",
    "print(\n",
    "    \"\\n\"\n",
    "    \"Filesystem, naming, and structural invariants\\n\"\n",
    "    \"are intact.\\n\\n\"\n",
    "    \"It is now safe to:\\n\"\n",
    "    \"- start new runs\\n\"\n",
    "    \"- generate embeddings\\n\"\n",
    "    \"- perform segmentation\\n\"\n",
    "    \"- write derived artifacts\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies (Colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow numpy matplotlib tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup: remove baseline image metadata outputs\n",
    "\n",
    "Deletes only `input_image.json` under each `plates_structured/<plate_id>/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "plates_structured = DATASET_ROOT / \"plates_structured\"\n",
    "\n",
    "removed = 0\n",
    "\n",
    "for plate_dir in plates_structured.iterdir():\n",
    "    if not plate_dir.is_dir():\n",
    "        continue\n",
    "    target = plate_dir / \"input_image.json\"\n",
    "    if target.exists():\n",
    "        target.unlink()\n",
    "        removed += 1\n",
    "\n",
    "print(\"Cleanup complete.\")\n",
    "print(f\"Removed input_image.json files: {removed}\")\n",
    "print(\"Dataset restored to pre-baseline state.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo-wide exploratory (read-only, CPU-only)\n",
    "\n",
    "Header-only inspection of source images; no writes and no full pixel loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "ROOT = DATASET_ROOT\n",
    "PLATES_STRUCTURED = ROOT / \"plates_structured\"\n",
    "LEDGER_DIR = ROOT / \"ledger\"\n",
    "SCHEMA_DIR = ROOT / \"schemas\"\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"REPO EXPLORATORY REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "top_dirs = sorted([p.name for p in ROOT.iterdir() if p.is_dir()])\n",
    "top_files = sorted([p.name for p in ROOT.iterdir() if p.is_file()])\n",
    "\n",
    "print(\"[1] Top-level directories:\")\n",
    "for d in top_dirs:\n",
    "    print(\"  -\", d)\n",
    "\n",
    "print(\"\\n[1] Top-level files:\")\n",
    "for f in top_files:\n",
    "    print(\"  -\", f)\n",
    "\n",
    "plate_dirs = sorted(\n",
    "    p for p in PLATES_STRUCTURED.iterdir()\n",
    "    if p.is_dir() and p.name.startswith(\"plate-\")\n",
    ")\n",
    "\n",
    "print(f\"\\n[2] Plates found: {len(plate_dirs)}\")\n",
    "\n",
    "missing_manifest = []\n",
    "missing_source = []\n",
    "run_counts = []\n",
    "\n",
    "for p in plate_dirs:\n",
    "    if not (p / \"manifest.json\").exists():\n",
    "        missing_manifest.append(p.name)\n",
    "\n",
    "    src_dir = p / \"source\"\n",
    "    if not src_dir.exists():\n",
    "        missing_source.append(p.name)\n",
    "    else:\n",
    "        files = list(src_dir.iterdir())\n",
    "        if len(files) != 1:\n",
    "            missing_source.append(p.name)\n",
    "\n",
    "    runs_dir = p / \"runs\"\n",
    "    if runs_dir.exists():\n",
    "        run_counts.append(len([r for r in runs_dir.iterdir() if r.is_dir()]))\n",
    "    else:\n",
    "        run_counts.append(0)\n",
    "\n",
    "print(\"    Missing manifest.json:\", len(missing_manifest))\n",
    "print(\"    Missing/invalid source:\", len(missing_source))\n",
    "print(\"    Plates with runs:\", sum(1 for c in run_counts if c > 0))\n",
    "print(\"    Total runs:\", sum(run_counts))\n",
    "\n",
    "print(\"\\n[3] Manifest field coverage (sampled)\")\n",
    "\n",
    "key_counter = Counter()\n",
    "sampled = 0\n",
    "\n",
    "for p in plate_dirs[:50]:\n",
    "    try:\n",
    "        m = json.loads((p / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "        key_counter.update(m.keys())\n",
    "        sampled += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for k, v in key_counter.most_common():\n",
    "    print(f\"  {k:20s} -> present in {v}/{sampled}\")\n",
    "\n",
    "print(\"\\n[4] Image header stats (source images only)\")\n",
    "\n",
    "sizes = []\n",
    "huge = []\n",
    "\n",
    "for p in plate_dirs:\n",
    "    try:\n",
    "        src = next((p / \"source\").iterdir())\n",
    "        with Image.open(src) as img:\n",
    "            w, h = img.size\n",
    "        mp = (w * h) / 1_000_000\n",
    "        sizes.append(mp)\n",
    "        if mp > 90:\n",
    "            huge.append((p.name, round(mp, 2)))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if sizes:\n",
    "    print(f\"    Min megapixels: {round(min(sizes), 2)}\")\n",
    "    print(f\"    Max megapixels: {round(max(sizes), 2)}\")\n",
    "    print(f\"    Mean megapixels: {round(sum(sizes)/len(sizes), 2)}\")\n",
    "    print(f\"    Images > 90 MP (PIL warning risk): {len(huge)}\")\n",
    "\n",
    "for name, mp in huge[:5]:\n",
    "    print(f\"      - {name}: {mp} MP\")\n",
    "\n",
    "print(\"\\n[5] Ledger files\")\n",
    "if LEDGER_DIR.exists():\n",
    "    for p in sorted(LEDGER_DIR.iterdir()):\n",
    "        if p.is_file():\n",
    "            print(f\"  - {p.name:20s} {round(p.stat().st_size/1024, 1)} KB\")\n",
    "else:\n",
    "    print(\"  Ledger directory missing\")\n",
    "\n",
    "print(\"\\n[5] Schemas\")\n",
    "if SCHEMA_DIR.exists():\n",
    "    for p in sorted(SCHEMA_DIR.iterdir()):\n",
    "        print(\"  -\", p.name)\n",
    "else:\n",
    "    print(\"  Schema directory missing\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"==============================\")\n",
    "print(\n",
    "    f\"\\n\"\n",
    "    f\"Plates                : {len(plate_dirs)}\\n\"\n",
    "    f\"Total runs            : {sum(run_counts)}\\n\"\n",
    "    f\"Images > 90 MP        : {len(huge)}\\n\"\n",
    "    f\"Manifests missing     : {len(missing_manifest)}\\n\"\n",
    "    f\"Source issues         : {len(missing_source)}\\n\"\n",
    "    f\"Schemas present       : {SCHEMA_DIR.exists()}\\n\"\n",
    "    f\"Ledgers present       : {LEDGER_DIR.exists()}\\n\"\n",
    ")\n",
    "\n",
    "print(\"Exploratory complete. No files written.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
