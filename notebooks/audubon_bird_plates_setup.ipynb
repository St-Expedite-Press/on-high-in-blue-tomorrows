{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audubon Bird Plates - Dataset Setup\n",
    "\n",
    "This notebook is an idempotent scaffolding + validation workflow for the Audubon bird plates dataset.\n",
    "\n",
    "It performs:\n",
    "- Dataset discovery (Colab Drive or local folder)\n",
    "- Disk/JSON sanity checks (expects 435 plates)\n",
    "- Creation of a structured per-plate directory layout (`plates_structured/plate-###/...`)\n",
    "- Per-plate `manifest.json` generation\n",
    "- JSON Schema authoring + validation\n",
    "- Per-plate source image SHA-256 checksums (`source.sha256`)\n",
    "- Run-system helpers (`runs/run-.../metrics.json`)\n",
    "- Ledger scaffolding (empty Parquet files with schemas)\n",
    "- Read-only system validation report\n",
    "\n",
    "Dataset root resolution:\n",
    "- Colab: mounts Drive and searches under `/content/drive/MyDrive/burning-world-series/`\n",
    "- Local: searches upward from the current working directory for a folder starting with `audubon-bird-plates`\n",
    "- Override: set `BWS_DATASET_ROOT` to an explicit path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install jsonschema pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Locate dataset\n",
    "\n",
    "This step resolves `DATASET_ROOT` (Colab or local) and sets canonical paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def find_dataset_root(start: Path) -> Path:\n",
    "    override = os.environ.get(\"BWS_DATASET_ROOT\")\n",
    "    if override:\n",
    "        p = Path(override).expanduser()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise RuntimeError(f\"BWS_DATASET_ROOT does not exist: {p}\")\n",
    "\n",
    "    def is_dataset_dir(p: Path) -> bool:\n",
    "        if not p.is_dir():\n",
    "            return False\n",
    "        if not p.name.startswith(\"audubon-bird-plates\"):\n",
    "            return False\n",
    "        if not (p / \"plates\").exists():\n",
    "            return False\n",
    "        if not (p / \"data.json\").exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    if is_dataset_dir(start):\n",
    "        return start\n",
    "\n",
    "    for base in [start] + list(start.parents):\n",
    "        try:\n",
    "            children = list(base.iterdir())\n",
    "        except Exception:\n",
    "            continue\n",
    "        for child in sorted(children):\n",
    "            if is_dataset_dir(child):\n",
    "                return child\n",
    "\n",
    "    raise RuntimeError(\"Could not find dataset root; set BWS_DATASET_ROOT\")\n",
    "\n",
    "if in_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    project_root = Path(\"/content/drive/MyDrive/burning-world-series\")\n",
    "    assert project_root.exists(), f\"Missing project root: {project_root}\"\n",
    "    DATASET_ROOT = find_dataset_root(project_root)\n",
    "else:\n",
    "    DATASET_ROOT = find_dataset_root(Path.cwd())\n",
    "\n",
    "PLATES_ROOT = DATASET_ROOT / \"plates\"\n",
    "DATA_JSON = DATASET_ROOT / \"data.json\"\n",
    "README_MD = DATASET_ROOT / \"README.md\"\n",
    "\n",
    "assert PLATES_ROOT.exists(), f\"Missing plates/: {PLATES_ROOT}\"\n",
    "assert DATA_JSON.exists(), f\"Missing data.json: {DATA_JSON}\"\n",
    "\n",
    "print(\"Dataset root:\", DATASET_ROOT)\n",
    "print(\"Plates root :\", PLATES_ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Index plates + validate metadata\n",
    "\n",
    "Build an in-memory index of `plates/plate-*.jpg`, then confirm `data.json` references only files that exist on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "plate_files = {\n",
    "    p.name: p\n",
    "    for p in PLATES_ROOT.rglob(\"plate-*.jpg\")\n",
    "}\n",
    "\n",
    "print(f\"Indexed {len(plate_files)} plate images on disk.\")\n",
    "for k in list(plate_files.keys())[:10]:\n",
    "    print(\" \", k, \"->\", plate_files[k])\n",
    "\n",
    "with open(DATA_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "assert isinstance(metadata, list)\n",
    "assert len(metadata) == 435, f\"Expected 435 entries, got {len(metadata)}\"\n",
    "\n",
    "PLATE_FILENAME_RE = re.compile(r\"^plate-(\\d+)(?:-[a-z0-9-]+)?\\.jpg$\")\n",
    "bad_filenames = []\n",
    "bad_plate_numbers = []\n",
    "for e in metadata:\n",
    "    m = PLATE_FILENAME_RE.match(e[\"fileName\"])\n",
    "    if not m:\n",
    "        bad_filenames.append(e[\"fileName\"])\n",
    "        continue\n",
    "    if int(m.group(1)) != int(e[\"plate\"]):\n",
    "        bad_plate_numbers.append((e[\"plate\"], e[\"fileName\"]))\n",
    "assert not bad_filenames, f\"Bad plate filenames (sample): {bad_filenames[:10]}\"\n",
    "assert not bad_plate_numbers, f\"Plate/fileName mismatch (sample): {bad_plate_numbers[:10]}\"\n",
    "\n",
    "missing = []\n",
    "for entry in metadata:\n",
    "    fname = entry[\"fileName\"]\n",
    "    if fname not in plate_files:\n",
    "        missing.append(fname)\n",
    "\n",
    "print(f\"Missing files referenced in JSON: {len(missing)}\")\n",
    "if missing:\n",
    "    for m in missing[:10]:\n",
    "        print(\" \", m)\n",
    "    raise RuntimeError(\"Metadata/disk mismatch\")\n",
    "\n",
    "print(\"All metadata filenames resolved on disk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create `plates_structured/` + per-plate manifests\n",
    "\n",
    "Creates a stable directory layout for each plate and writes a minimal `manifest.json` once.\n",
    "\n",
    "- Safe to re-run: existing manifests are reused.\n",
    "- Source images are copied once into `source/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "STRUCTURED_ROOT = DATASET_ROOT / \"plates_structured\"\n",
    "STRUCTURED_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "def plate_id(n: int) -> str:\n",
    "    return f\"plate-{str(n).zfill(3)}\"\n",
    "\n",
    "created = 0\n",
    "kept = 0\n",
    "\n",
    "for entry in metadata:\n",
    "    plate_num = int(entry[\"plate\"])\n",
    "    pid = plate_id(plate_num)\n",
    "\n",
    "    plate_dir  = STRUCTURED_ROOT / pid\n",
    "    source_dir = plate_dir / \"source\"\n",
    "    runs_dir   = plate_dir / \"runs\"\n",
    "    viz_dir    = plate_dir / \"viz\"\n",
    "    cache_dir  = plate_dir / \"cache\"\n",
    "\n",
    "    for d in (source_dir, runs_dir, viz_dir, cache_dir):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    src_path = plate_files.get(entry[\"fileName\"])\n",
    "    canonical_name = f\"{pid}.jpg\"\n",
    "    dst_path = source_dir / canonical_name\n",
    "    canonical_rel = f\"source/{canonical_name}\"\n",
    "\n",
    "    manifest_path = plate_dir / \"manifest.json\"\n",
    "    if manifest_path.exists():\n",
    "        manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "        src_rel = manifest.get(\"source_image\")\n",
    "        src_existing = plate_dir / src_rel if isinstance(src_rel, str) else None\n",
    "\n",
    "        if not dst_path.exists():\n",
    "            if src_existing and src_existing.exists() and src_existing.parent == source_dir:\n",
    "                src_existing.rename(dst_path)\n",
    "            else:\n",
    "                files = [p for p in source_dir.iterdir() if p.is_file()]\n",
    "                if len(files) == 1:\n",
    "                    files[0].rename(dst_path)\n",
    "                elif len(files) == 0:\n",
    "                    if src_path is None:\n",
    "                        raise RuntimeError(f\"Missing source image on disk: {entry['fileName']}\")\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"{pid}: source/ contains {len(files)} files\")\n",
    "\n",
    "        if manifest.get(\"source_image\") != canonical_rel:\n",
    "            manifest[\"source_image\"] = canonical_rel\n",
    "            manifest_path.write_text(\n",
    "                json.dumps(manifest, indent=2, ensure_ascii=False),\n",
    "                encoding=\"utf-8\",\n",
    "            )\n",
    "\n",
    "        kept += 1\n",
    "        continue\n",
    "\n",
    "    if not dst_path.exists():\n",
    "        if src_path is None:\n",
    "            raise RuntimeError(f\"Missing source image on disk: {entry['fileName']}\")\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    manifest = {\n",
    "        \"plate_id\": pid,\n",
    "        \"plate_number\": plate_num,\n",
    "        \"title\": entry[\"name\"],\n",
    "        \"slug\": entry[\"slug\"],\n",
    "        \"source_image\": canonical_rel,\n",
    "        \"download_url\": entry.get(\"download\"),\n",
    "        \"license\": \"Public Domain\",\n",
    "        \"created_at\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }\n",
    "\n",
    "    manifest_path.write_text(\n",
    "        json.dumps(manifest, indent=2, ensure_ascii=False),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    created += 1\n",
    "\n",
    "print(f\"Plates created : {created}\")\n",
    "print(f\"Plates reused : {kept}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Write JSON Schemas\n",
    "\n",
    "Schemas are persisted under `schemas/` so tooling can validate manifests and run metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "SCHEMA_DIR = DATASET_ROOT / \"schemas\"\n",
    "SCHEMA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLATE_MANIFEST_SCHEMA = {\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"https://burning-world-series/schemas/plate.manifest.schema.json\",\n",
    "    \"title\": \"Audubon Plate Manifest\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\n",
    "        \"plate_id\",\n",
    "        \"plate_number\",\n",
    "        \"title\",\n",
    "        \"slug\",\n",
    "        \"source_image\",\n",
    "    ],\n",
    "    \"properties\": {\n",
    "        \"plate_id\": {\"type\": \"string\", \"pattern\": \"^plate-[0-9]{3}$\"},\n",
    "        \"plate_number\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 435},\n",
    "        \"title\": {\"type\": \"string\"},\n",
    "        \"slug\": {\"type\": \"string\"},\n",
    "        \"source_image\": {\"type\": \"string\"},\n",
    "        \"download_url\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"license\": {\"type\": \"string\"},\n",
    "        \"created_at\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "    },\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "RUN_MANIFEST_SCHEMA = {\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"https://burning-world-series/schemas/run.manifest.schema.json\",\n",
    "    \"title\": \"Plate Run Manifest\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"run_id\", \"plate_id\", \"timestamp\", \"models\", \"outputs\"],\n",
    "    \"properties\": {\n",
    "        \"run_id\": {\"type\": \"string\"},\n",
    "        \"plate_id\": {\"type\": \"string\"},\n",
    "        \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "        \"models\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"outputs\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"notes\": {\"type\": [\"string\", \"null\"]},\n",
    "    },\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "def write_schema(path: Path, schema: dict):\n",
    "    path.write_text(json.dumps(schema, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "write_schema(SCHEMA_DIR / \"plate.manifest.schema.json\", PLATE_MANIFEST_SCHEMA)\n",
    "write_schema(SCHEMA_DIR / \"run.manifest.schema.json\", RUN_MANIFEST_SCHEMA)\n",
    "\n",
    "print(\"Schemas written to:\", SCHEMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Validate manifests + sources\n",
    "\n",
    "Validates every plate's `manifest.json` against the schema, and confirms the referenced source image exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonschema import Draft202012Validator\n",
    "\n",
    "SCHEMA_PATH = SCHEMA_DIR / \"plate.manifest.schema.json\"\n",
    "plate_schema = json.loads(SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "validator = Draft202012Validator(plate_schema)\n",
    "\n",
    "errors = []\n",
    "checked = 0\n",
    "\n",
    "for plate_dir in (DATASET_ROOT / \"plates_structured\").iterdir():\n",
    "    if not plate_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    manifest_path = plate_dir / \"manifest.json\"\n",
    "    if not manifest_path.exists():\n",
    "        errors.append(f\"{plate_dir.name}: missing manifest.json\")\n",
    "        continue\n",
    "\n",
    "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    for err in validator.iter_errors(manifest):\n",
    "        errors.append(f\"{plate_dir.name}: {list(err.path)} -> {err.message}\")\n",
    "\n",
    "    src = plate_dir / manifest[\"source_image\"]\n",
    "    if not src.exists():\n",
    "        errors.append(f\"{plate_dir.name}: missing source image {src}\")\n",
    "\n",
    "    checked += 1\n",
    "\n",
    "print(f\"Validated {checked} plate manifests.\")\n",
    "\n",
    "if errors:\n",
    "    print(\"\\nVALIDATION ERRORS:\")\n",
    "    for e in errors[:20]:\n",
    "        print(\" \", e)\n",
    "    raise RuntimeError(f\"{len(errors)} validation errors found.\")\n",
    "else:\n",
    "    print(\"All plate manifests are valid.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Write / verify per-plate source checksums\n",
    "\n",
    "Creates `source.sha256` in each plate directory for integrity checks.\n",
    "\n",
    "- If `source.sha256` exists, it's verified.\n",
    "- If missing, it's created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def sha256(path: Path, chunk_size: int = 8192) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "PLATES_STRUCTURED = DATASET_ROOT / \"plates_structured\"\n",
    "\n",
    "checksums_written = 0\n",
    "verified = 0\n",
    "\n",
    "for plate_dir in PLATES_STRUCTURED.iterdir():\n",
    "    if not plate_dir.is_dir():\n",
    "        continue\n",
    "\n",
    "    manifest_path = plate_dir / \"manifest.json\"\n",
    "    if not manifest_path.exists():\n",
    "        continue\n",
    "\n",
    "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    src_path = plate_dir / manifest[\"source_image\"]\n",
    "\n",
    "    checksum_path = plate_dir / \"source.sha256\"\n",
    "    digest = sha256(src_path)\n",
    "\n",
    "    if checksum_path.exists():\n",
    "        recorded = checksum_path.read_text().strip()\n",
    "        if recorded != digest:\n",
    "            raise RuntimeError(\n",
    "                f\"CHECKSUM MISMATCH: {plate_dir.name}\\n\"\n",
    "                f\" recorded: {recorded}\\n\"\n",
    "                f\" current : {digest}\"\n",
    "            )\n",
    "        verified += 1\n",
    "    else:\n",
    "        checksum_path.write_text(digest)\n",
    "        checksums_written += 1\n",
    "\n",
    "print(f\"Checksums written : {checksums_written}\")\n",
    "print(f\"Checksums verified: {verified}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Run system helpers\n",
    "\n",
    "Defines a tiny run system for append-only processing outputs under each plate's `runs/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def generate_run_id(models: list, note: str = \"\") -> str:\n",
    "    \\\"\\\"\\\"\n",
    "    Generate a sortable, collision-resistant run ID.\n",
    "    Format: run-YYYYMMDD-HHMMSS-<hash>\n",
    "    \\\"\\\"\\\"\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    payload = \"|\".join(models) + \"|\" + note\n",
    "    short = hashlib.sha1(payload.encode()).hexdigest()[:8]\n",
    "    return f\"run-{stamp}-{short}\"\n",
    "\n",
    "def start_run(plate_dir: Path, models: list, note: str = \"\") -> Path:\n",
    "    \\\"\\\"\\\"\n",
    "    Initialize a new append-only run directory and manifest.\n",
    "    Writes runs/<run_id>/metrics.json.\n",
    "    \\\"\\\"\\\"\n",
    "    run_id = generate_run_id(models, note)\n",
    "    run_dir = plate_dir / \"runs\" / run_id\n",
    "\n",
    "    if run_dir.exists():\n",
    "        raise RuntimeError(f\"Run already exists: {run_dir}\")\n",
    "\n",
    "    run_dir.mkdir(parents=True)\n",
    "\n",
    "    manifest = {\n",
    "        \"run_id\": run_id,\n",
    "        \"plate_id\": plate_dir.name,\n",
    "        \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"models\": models,\n",
    "        \"outputs\": [],\n",
    "        \"notes\": note or None,\n",
    "    }\n",
    "\n",
    "    (run_dir / \"metrics.json\").write_text(\n",
    "        json.dumps(manifest, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    return run_dir\n",
    "\n",
    "def register_output(run_dir: Path, relative_path: str):\n",
    "    \\\"\\\"\\\"\n",
    "    Register an output artifact relative to the run directory.\n",
    "    Updates runs/<run_id>/metrics.json.\n",
    "    \\\"\\\"\\\"\n",
    "    manifest_path = run_dir / \"metrics.json\"\n",
    "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if relative_path not in manifest[\"outputs\"]:\n",
    "        manifest[\"outputs\"].append(relative_path)\n",
    "\n",
    "    manifest_path.write_text(\n",
    "        json.dumps(manifest, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Ledger initialization (derived, rebuildable)\n",
    "\n",
    "Creates **empty** Parquet files with the correct schemas.\n",
    "\n",
    "- Safe to re-run.\n",
    "- Does **not** populate ledgers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "LEDGER_DIR = DATASET_ROOT / \"ledger\"\n",
    "LEDGER_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "PLATES_SCHEMA = pa.schema([\n",
    "    (\"plate_id\", pa.string()),\n",
    "    (\"plate_number\", pa.int16()),\n",
    "    (\"title\", pa.string()),\n",
    "    (\"source_checksum\", pa.string()),\n",
    "])\n",
    "\n",
    "RUNS_SCHEMA = pa.schema([\n",
    "    (\"run_id\", pa.string()),\n",
    "    (\"plate_id\", pa.string()),\n",
    "    (\"timestamp\", pa.timestamp(\"ms\", tz=\"UTC\")),\n",
    "    (\"models\", pa.list_(pa.string())),\n",
    "    (\"notes\", pa.string()),\n",
    "])\n",
    "\n",
    "EMBEDDINGS_SCHEMA = pa.schema([\n",
    "    (\"run_id\", pa.string()),\n",
    "    (\"plate_id\", pa.string()),\n",
    "    (\"model\", pa.string()),\n",
    "    (\"vector\", pa.list_(pa.float32())),\n",
    "])\n",
    "\n",
    "SEGMENTS_SCHEMA = pa.schema([\n",
    "    (\"run_id\", pa.string()),\n",
    "    (\"plate_id\", pa.string()),\n",
    "    (\"segment_id\", pa.string()),\n",
    "    (\"area_ratio\", pa.float32()),\n",
    "])\n",
    "\n",
    "SCHEMAS = {\n",
    "    \"plates.parquet\": PLATES_SCHEMA,\n",
    "    \"runs.parquet\": RUNS_SCHEMA,\n",
    "    \"embeddings.parquet\": EMBEDDINGS_SCHEMA,\n",
    "    \"segments.parquet\": SEGMENTS_SCHEMA,\n",
    "}\n",
    "\n",
    "for name, schema in SCHEMAS.items():\n",
    "    path = LEDGER_DIR / name\n",
    "    if not path.exists():\n",
    "        empty_table = pa.Table.from_arrays(\n",
    "            [pa.array([], type=field.type) for field in schema],\n",
    "            schema=schema,\n",
    "        )\n",
    "        pq.write_table(empty_table, path)\n",
    "        print(f\"Created {name}\")\n",
    "    else:\n",
    "        print(f\"Exists  {name}\")\n",
    "\n",
    "print(\"Ledger initialization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) System validation report (read-only)\n",
    "\n",
    "This section asserts core invariants without mutating state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from jsonschema import Draft202012Validator\n",
    "\n",
    "PLATE_DIR_RE = re.compile(r\"^plate-[0-9]{3}$\")\n",
    "PLATE_FILENAME_RE = re.compile(r\"^plate-[0-9]{3}\\.jpg$\")\n",
    "RUN_DIR_RE = re.compile(r\"^run-[0-9]{8}-[0-9]{6}-[0-9a-f]{8}$\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"SYSTEM VALIDATION REPORT\")\n",
    "print(\"==============================\\n\")\n",
    "\n",
    "PLATES_STRUCTURED = DATASET_ROOT / \"plates_structured\"\n",
    "plates = sorted([p for p in PLATES_STRUCTURED.iterdir() if p.is_dir()])\n",
    "\n",
    "print(f\"[1] Plates structured: {len(plates)}\")\n",
    "assert len(plates) == 435, \"Plate count mismatch (expected 435)\"\n",
    "\n",
    "bad_names = [p.name for p in plates if not PLATE_DIR_RE.match(p.name)]\n",
    "assert not bad_names, f\"Bad plate directory names: {bad_names[:5]}\"\n",
    "print(\"    OK Plate directory count and naming OK\")\n",
    "\n",
    "PLATE_SCHEMA_PATH = DATASET_ROOT / \"schemas\" / \"plate.manifest.schema.json\"\n",
    "RUN_SCHEMA_PATH = DATASET_ROOT / \"schemas\" / \"run.manifest.schema.json\"\n",
    "\n",
    "assert PLATE_SCHEMA_PATH.exists(), \"Missing plate.manifest.schema.json\"\n",
    "assert RUN_SCHEMA_PATH.exists(), \"Missing run.manifest.schema.json\"\n",
    "print(\"    OK Schemas present\")\n",
    "\n",
    "plate_schema = json.loads(PLATE_SCHEMA_PATH.read_text(encoding=\"utf-8\"))\n",
    "validator = Draft202012Validator(plate_schema)\n",
    "\n",
    "missing = []\n",
    "schema_errors = []\n",
    "checksum_missing = 0\n",
    "\n",
    "for plate_dir in plates:\n",
    "    manifest_path = plate_dir / \"manifest.json\"\n",
    "    source_dir = plate_dir / \"source\"\n",
    "    checksum_path = plate_dir / \"source.sha256\"\n",
    "\n",
    "    if not manifest_path.exists():\n",
    "        missing.append(f\"{plate_dir.name}: missing manifest\")\n",
    "        continue\n",
    "\n",
    "    manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if manifest.get(\"plate_id\") != plate_dir.name:\n",
    "        missing.append(f\"{plate_dir.name}: manifest plate_id mismatch ({manifest.get(\"plate_id\")})\")\n",
    "\n",
    "    expected_plate_number = int(plate_dir.name.split(\"-\")[1])\n",
    "    if manifest.get(\"plate_number\") != expected_plate_number:\n",
    "        missing.append(f\"{plate_dir.name}: manifest plate_number mismatch ({manifest.get(\"plate_number\")})\")\n",
    "\n",
    "    src_rel = manifest.get(\"source_image\")\n",
    "    if not isinstance(src_rel, str) or not src_rel.startswith(\"source/\"):\n",
    "        missing.append(f\"{plate_dir.name}: bad source_image path ({src_rel})\")\n",
    "        src_name = None\n",
    "    else:\n",
    "        src_name = src_rel.split(\"/\", 1)[1]\n",
    "        if not PLATE_FILENAME_RE.match(src_name):\n",
    "            missing.append(f\"{plate_dir.name}: bad source filename ({src_name})\")\n",
    "        expected_src_name = f\"plate-{expected_plate_number:03d}.jpg\"\n",
    "        if src_name != expected_src_name:\n",
    "            missing.append(f\"{plate_dir.name}: source filename mismatch ({src_name} != {expected_src_name})\")\n",
    "\n",
    "    for err in validator.iter_errors(manifest):\n",
    "        schema_errors.append(f\"{plate_dir.name}: {list(err.path)} -> {err.message}\")\n",
    "\n",
    "    src = plate_dir / manifest[\"source_image\"]\n",
    "    if not src.exists():\n",
    "        missing.append(f\"{plate_dir.name}: missing source image\")\n",
    "\n",
    "    images = list(source_dir.glob(\"*\"))\n",
    "    if len(images) != 1:\n",
    "        missing.append(f\"{plate_dir.name}: source/ contains {len(images)} files\")\n",
    "    elif src_name is not None and images[0].name != src_name:\n",
    "        missing.append(f\"{plate_dir.name}: source/ filename mismatch ({images[0].name} != {src_name})\")\n",
    "\n",
    "    runs_root = plate_dir / \"runs\"\n",
    "    if runs_root.exists():\n",
    "        for run_dir in runs_root.iterdir():\n",
    "            if run_dir.is_dir() and not run_dir.name.startswith(\".\") and not RUN_DIR_RE.match(run_dir.name):\n",
    "                missing.append(f\"{plate_dir.name}: bad run dir name {run_dir.name}\")\n",
    "\n",
    "    if not checksum_path.exists():\n",
    "        checksum_missing += 1\n",
    "\n",
    "assert not missing, f\"Missing or malformed plate artifacts:\\n{missing[:5]}\"\n",
    "assert not schema_errors, f\"Schema errors detected:\\n{schema_errors[:5]}\"\n",
    "assert checksum_missing == 0, f\"{checksum_missing} plates missing checksums\"\n",
    "print(\"    OK Plate manifests, sources, and checksums OK\")\n",
    "\n",
    "LEDGER_DIR = DATASET_ROOT / \"ledger\"\n",
    "expected_ledgers = {\"plates.parquet\", \"runs.parquet\", \"embeddings.parquet\", \"segments.parquet\"}\n",
    "found_ledgers = {p.name for p in LEDGER_DIR.iterdir() if p.is_file()}\n",
    "missing_ledgers = expected_ledgers - found_ledgers\n",
    "assert not missing_ledgers, f\"Missing ledger files: {missing_ledgers}\"\n",
    "print(\"    OK Ledger scaffolding present\")\n",
    "\n",
    "assert callable(generate_run_id)\n",
    "assert callable(start_run)\n",
    "assert callable(register_output)\n",
    "\n",
    "test_run_id = generate_run_id([\"test-model\"], \"dry-run\")\n",
    "assert test_run_id.startswith(\"run-\")\n",
    "print(\"    OK Run system callable (dry)\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"STATUS: SYSTEM IS CONSISTENT\")\n",
    "print(\"==============================\")\n",
    "print(\n",
    "    \"\\n\"\n",
    "    \"- 435 plate directories present\\n\"\n",
    "    \"- All manifests schema-valid\\n\"\n",
    "    \"- All source images present and checksummed\\n\"\n",
    "    \"- Schemas persisted\\n\"\n",
    "    \"- Ledgers scaffolded\\n\"\n",
    "    \"- Run system ready but unused\\n\\n\"\n",
    "    \"NEXT SAFE ACTION:\\n\"\n",
    "    \"- Start your FIRST REAL RUN (segmentation or embeddings)\\n\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
