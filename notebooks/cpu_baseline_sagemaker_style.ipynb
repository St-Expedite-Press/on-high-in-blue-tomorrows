{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU Baseline (SageMaker-style: read-only input, write-only output)\n",
    "\n",
    "This notebook is designed to run on local CPU while matching the SageMaker mental model:\n",
    "\n",
    "- Treat `INPUT_ROOT` as **read-only**.\n",
    "- Write all new artifacts under `OUTPUT_ROOT`.\n",
    "- Use shard parameters so the same code can scale to SageMaker Processing.\n",
    "\n",
    "## Input criteria (must already be true)\n",
    "\n",
    "`INPUT_ROOT/` must contain:\n",
    "\n",
    "- `plates_structured/` with exactly 435 plate directories named `plate-###`\n",
    "- For each plate directory:\n",
    "  - `manifest.json`\n",
    "  - `source.sha256`\n",
    "  - `source/plate-###.jpg`\n",
    "- `schemas/plate.manifest.schema.json`\n",
    "- `schemas/run.manifest.schema.json`\n",
    "\n",
    "## Midpoint artifacts (written once)\n",
    "\n",
    "- `OUTPUT_ROOT/schemas/cpu.baseline.schema.json`\n",
    "- `OUTPUT_ROOT/reports/<run_id>/report.json`\n",
    "\n",
    "## Outputs (append-only, per plate)\n",
    "\n",
    "For each plate in the shard:\n",
    "\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/metrics.json`\n",
    "- `OUTPUT_ROOT/plates_structured/<plate_id>/runs/<run_id>/cpu_baseline.json`\n",
    "\n",
    "## Constraints enforced\n",
    "\n",
    "- No mutation of `INPUT_ROOT`.\n",
    "- Append-only run outputs in `OUTPUT_ROOT`.\n",
    "- `metrics.json` validates against `schemas/run.manifest.schema.json`.\n",
    "- `cpu_baseline.json` validates against `OUTPUT_ROOT/schemas/cpu.baseline.schema.json`.\n",
    "\n",
    "## CPU-only feature pack\n",
    "\n",
    "- Container facts: bytes, extension, format, sha256, EXIF presence, ICC presence+hash, JPEG progressive/subsampling/quant hash when exposed\n",
    "- Geometry: width, height, megapixels, aspect ratio, tiling lattice\n",
    "- Pixel distribution: RGB histograms and luma histogram (256 bins)\n",
    "- Summary stats: mean/std/min/max + clipped ratios per channel + luma\n",
    "- Entropy per channel + luma\n",
    "- Hashes: aHash, dHash, pHash\n",
    "- Sharpness proxy: Laplacian variance on downsampled grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from jsonschema import Draft202012Validator\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "TILE_SIZE = 512\n",
    "HIST_BINS = 256\n",
    "LAPLACE_MAX_DIM = 1024\n",
    "PHASH_BITS = 8\n",
    "PHASH_FACTOR = 4\n",
    "\n",
    "def utc_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def generate_run_id(models: list[str], note: str) -> str:\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "    payload = \"|\".join(models) + \"|\" + note\n",
    "    short = hashlib.sha1(payload.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return f\"run-{stamp}-{short}\"\n",
    "\n",
    "def resize_fit(img: Image.Image, max_dim: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    m = max(w, h)\n",
    "    if m <= max_dim:\n",
    "        return img\n",
    "    s = max_dim / m\n",
    "    return img.resize((max(1, int(round(w * s))), max(1, int(round(h * s)))), resample=Image.BILINEAR)\n",
    "\n",
    "def laplacian_variance(gray_img: Image.Image) -> float:\n",
    "    g = resize_fit(gray_img, LAPLACE_MAX_DIM)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    ap = np.pad(a, ((1, 1), (1, 1)), mode=\"edge\")\n",
    "    out = ap[:-2, 1:-1] + ap[2:, 1:-1] + ap[1:-1, :-2] + ap[1:-1, 2:] - 4.0 * ap[1:-1, 1:-1]\n",
    "    return float(np.var(out))\n",
    "\n",
    "def hist_stats(hist: list[int]) -> dict:\n",
    "    total = int(sum(hist))\n",
    "    if total <= 0:\n",
    "        return {\"total\": 0, \"min\": None, \"max\": None, \"mean\": None, \"std\": None, \"clipped_low_ratio\": None, \"clipped_high_ratio\": None}\n",
    "    mn = next((i for i, c in enumerate(hist) if c), 0)\n",
    "    mx = next((255 - i for i, c in enumerate(reversed(hist)) if c), 255)\n",
    "    mean = sum(i * c for i, c in enumerate(hist)) / total\n",
    "    s2 = sum((i - mean) ** 2 * c for i, c in enumerate(hist)) / total\n",
    "    return {\"total\": total, \"min\": int(mn), \"max\": int(mx), \"mean\": float(mean), \"std\": float(math.sqrt(s2)), \"clipped_low_ratio\": float(hist[0] / total), \"clipped_high_ratio\": float(hist[-1] / total)}\n",
    "\n",
    "def entropy(hist: list[int]) -> float | None:\n",
    "    total = float(sum(hist))\n",
    "    if total <= 0:\n",
    "        return None\n",
    "    ent = 0.0\n",
    "    for c in hist:\n",
    "        if c:\n",
    "            p = c / total\n",
    "            ent -= p * math.log2(p)\n",
    "    return float(ent)\n",
    "\n",
    "def ahash(img: Image.Image, size: int = 8) -> str:\n",
    "    g = img.convert(\"L\").resize((size, size), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    m = a.mean()\n",
    "    bits = (a > m).astype(np.uint8).flatten()\n",
    "    v = 0\n",
    "    for b in bits:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(size * size) // 4}x}\"\n",
    "\n",
    "def dhash(img: Image.Image, size: int = 8) -> str:\n",
    "    g = img.convert(\"L\").resize((size + 1, size), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    diff = (a[:, 1:] > a[:, :-1]).astype(np.uint8).flatten()\n",
    "    v = 0\n",
    "    for b in diff:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(size * size) // 4}x}\"\n",
    "\n",
    "def phash(img: Image.Image, hash_bits: int = PHASH_BITS, highfreq_factor: int = PHASH_FACTOR) -> str:\n",
    "    n = hash_bits * highfreq_factor\n",
    "    g = img.convert(\"L\").resize((n, n), Image.BILINEAR)\n",
    "    a = np.asarray(g, dtype=np.float32)\n",
    "    idx = np.arange(n)\n",
    "    k = idx.reshape(-1, 1)\n",
    "    cos = np.cos(np.pi / n * (idx + 0.5) * k).astype(np.float32)\n",
    "    d1 = cos @ a\n",
    "    d2 = (cos @ d1.T).T\n",
    "    d = d2[:hash_bits, :hash_bits].flatten()\n",
    "    med = np.median(d[1:]) if d.size > 1 else np.median(d)\n",
    "    bits = (d > med).astype(np.uint8)\n",
    "    v = 0\n",
    "    for b in bits:\n",
    "        v = (v << 1) | int(b)\n",
    "    return f\"{v:0{(hash_bits * hash_bits) // 4}x}\"\n",
    "\n",
    "def exif_present(img) -> bool:\n",
    "    try:\n",
    "        ex = img.getexif()\n",
    "        return ex is not None and len(ex) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def icc_hash(img):\n",
    "    try:\n",
    "        icc = img.info.get(\"icc_profile\", None)\n",
    "        if not icc:\n",
    "            return False, None\n",
    "        if isinstance(icc, str):\n",
    "            icc = icc.encode(\"utf-8\", errors=\"ignore\")\n",
    "        return True, hashlib.sha256(icc).hexdigest()\n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "def jpeg_progressive_flag(img):\n",
    "    v = img.info.get(\"progressive\", None)\n",
    "    return None if v is None else bool(v)\n",
    "\n",
    "def jpeg_subsampling_flag(img):\n",
    "    v = img.info.get(\"subsampling\", None)\n",
    "    return None if v is None else str(v)\n",
    "\n",
    "def quant_hash_from_pil(img):\n",
    "    q = getattr(img, \"quantization\", None)\n",
    "    if not q:\n",
    "        return None\n",
    "    try:\n",
    "        payload = json.dumps(q, sort_keys=True)\n",
    "        return hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_validator(path: Path) -> Draft202012Validator:\n",
    "    schema = json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "    Draft202012Validator.check_schema(schema)\n",
    "    return Draft202012Validator(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_input_root(start: Path) -> Path:\n",
    "    override = os.environ.get(\"BWS_INPUT_ROOT\")\n",
    "    if override:\n",
    "        p = Path(override).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise RuntimeError(f\"BWS_INPUT_ROOT does not exist: {p}\")\n",
    "\n",
    "    def is_scaffolded(p: Path) -> bool:\n",
    "        if not p.is_dir():\n",
    "            return False\n",
    "        if not (p / \"plates_structured\").exists():\n",
    "            return False\n",
    "        if not (p / \"schemas\" / \"plate.manifest.schema.json\").exists():\n",
    "            return False\n",
    "        if not (p / \"schemas\" / \"run.manifest.schema.json\").exists():\n",
    "            return False\n",
    "        if not (p / \"data.json\").exists():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    candidates = [start] + list(start.parents)\n",
    "    for base in candidates:\n",
    "        if is_scaffolded(base):\n",
    "            return base\n",
    "        try:\n",
    "            for child in sorted(base.iterdir()):\n",
    "                if is_scaffolded(child):\n",
    "                    return child\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    raise RuntimeError(\"Could not find scaffolded INPUT_ROOT; set BWS_INPUT_ROOT\")\n",
    "\n",
    "INPUT_ROOT = find_input_root(Path.cwd())\n",
    "OUTPUT_ROOT = Path(os.environ.get(\"BWS_OUTPUT_ROOT\", str(INPUT_ROOT / \"_RUN_OUTPUT\"))).expanduser().resolve()\n",
    "SHARD_INDEX = int(os.environ.get(\"BWS_SHARD_INDEX\", \"0\"))\n",
    "SHARD_COUNT = int(os.environ.get(\"BWS_SHARD_COUNT\", \"1\"))\n",
    "RUN_ID = os.environ.get(\"BWS_RUN_ID\", None)\n",
    "SKIP_IF_PRESENT = os.environ.get(\"BWS_SKIP_IF_PRESENT\", \"1\") == \"1\"\n",
    "\n",
    "print(\"INPUT_ROOT :\", INPUT_ROOT)\n",
    "print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n",
    "print(\"SHARD      :\", SHARD_INDEX, \"/\", SHARD_COUNT)\n",
    "print(\"RUN_ID     :\", RUN_ID)\n",
    "print(\"SKIP       :\", SKIP_IF_PRESENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLATES_ROOT = INPUT_ROOT / \"plates_structured\"\n",
    "SCHEMAS_ROOT = INPUT_ROOT / \"schemas\"\n",
    "\n",
    "plate_schema_path = SCHEMAS_ROOT / \"plate.manifest.schema.json\"\n",
    "run_schema_path = SCHEMAS_ROOT / \"run.manifest.schema.json\"\n",
    "\n",
    "if not plate_schema_path.exists():\n",
    "    raise RuntimeError(f\"Missing: {plate_schema_path}\")\n",
    "if not run_schema_path.exists():\n",
    "    raise RuntimeError(f\"Missing: {run_schema_path}\")\n",
    "\n",
    "plate_validator = load_validator(plate_schema_path)\n",
    "run_validator = load_validator(run_schema_path)\n",
    "\n",
    "plates = sorted([p for p in PLATES_ROOT.iterdir() if p.is_dir() and p.name.startswith(\"plate-\")])\n",
    "if len(plates) != 435:\n",
    "    raise RuntimeError(f\"Unexpected plate count: {len(plates)}\")\n",
    "\n",
    "selected = [p for i, p in enumerate(plates) if i % SHARD_COUNT == SHARD_INDEX]\n",
    "print(\"plates_total   :\", len(plates))\n",
    "print(\"plates_selected:\", len(selected))\n",
    "\n",
    "cpu_schema_dir = OUTPUT_ROOT / \"schemas\"\n",
    "cpu_schema_dir.mkdir(parents=True, exist_ok=True)\n",
    "cpu_schema_path = cpu_schema_dir / \"cpu.baseline.schema.json\"\n",
    "\n",
    "CPU_BASELINE_SCHEMA = {\n",
    "    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
    "    \"$id\": \"https://burning-world-series/schemas/cpu.baseline.schema.json\",\n",
    "    \"title\": \"CPU Baseline Feature Pack\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"plate_id\", \"run_id\", \"timestamp\", \"source_image\", \"source_file\", \"geometry\", \"tiling\", \"decode\"],\n",
    "    \"properties\": {\n",
    "        \"plate_id\": {\"type\": \"string\", \"pattern\": \"^plate-[0-9]{3}$\"},\n",
    "        \"run_id\": {\"type\": \"string\"},\n",
    "        \"timestamp\": {\"type\": \"string\", \"format\": \"date-time\"},\n",
    "        \"source_image\": {\"type\": \"string\"},\n",
    "        \"source_file\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"bytes\", \"extension\", \"format\", \"sha256\", \"exif_present\", \"icc_present\", \"icc_hash\", \"jpeg_is_progressive\", \"jpeg_subsampling\", \"jpeg_quant_hash\"],\n",
    "            \"properties\": {\n",
    "                \"bytes\": {\"type\": \"integer\", \"minimum\": 0},\n",
    "                \"extension\": {\"type\": \"string\"},\n",
    "                \"format\": {\"type\": [\"string\", \"null\"]},\n",
    "                \"sha256\": {\"type\": \"string\"},\n",
    "                \"exif_present\": {\"type\": \"boolean\"},\n",
    "                \"icc_present\": {\"type\": \"boolean\"},\n",
    "                \"icc_hash\": {\"type\": [\"string\", \"null\"]},\n",
    "                \"jpeg_is_progressive\": {\"type\": [\"boolean\", \"null\"]},\n",
    "                \"jpeg_subsampling\": {\"type\": [\"string\", \"null\"]},\n",
    "                \"jpeg_quant_hash\": {\"type\": [\"string\", \"null\"]}\n",
    "            },\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"geometry\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"width_px\", \"height_px\", \"megapixels\", \"aspect_ratio\", \"mode\"],\n",
    "            \"properties\": {\n",
    "                \"width_px\": {\"type\": [\"integer\", \"null\"], \"minimum\": 1},\n",
    "                \"height_px\": {\"type\": [\"integer\", \"null\"], \"minimum\": 1},\n",
    "                \"megapixels\": {\"type\": [\"number\", \"null\"], \"minimum\": 0},\n",
    "                \"aspect_ratio\": {\"type\": [\"number\", \"null\"], \"minimum\": 0},\n",
    "                \"mode\": {\"type\": [\"string\", \"null\"]}\n",
    "            },\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"tiling\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"tile_size_px\", \"tiles_x\", \"tiles_y\", \"total_tiles\"],\n",
    "            \"properties\": {\n",
    "                \"tile_size_px\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                \"tiles_x\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                \"tiles_y\": {\"type\": \"integer\", \"minimum\": 1},\n",
    "                \"total_tiles\": {\"type\": \"integer\", \"minimum\": 1}\n",
    "            },\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"decode\": {\n",
    "            \"type\": \"object\",\n",
    "            \"required\": [\"ok\", \"error\"],\n",
    "            \"properties\": {\"ok\": {\"type\": \"boolean\"}, \"error\": {\"type\": [\"string\", \"null\"]}},\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"pixel_stats\": {\n",
    "            \"type\": [\"object\", \"null\"],\n",
    "            \"properties\": {\n",
    "                \"colorspace\": {\"type\": \"string\"},\n",
    "                \"rgb_histograms\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\"bins\", \"R\", \"G\", \"B\"],\n",
    "                    \"properties\": {\n",
    "                        \"bins\": {\"type\": \"integer\"},\n",
    "                        \"R\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"minItems\": 256, \"maxItems\": 256},\n",
    "                        \"G\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"minItems\": 256, \"maxItems\": 256},\n",
    "                        \"B\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"minItems\": 256, \"maxItems\": 256}\n",
    "                    },\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"l_histogram\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\"bins\", \"L\"],\n",
    "                    \"properties\": {\n",
    "                        \"bins\": {\"type\": \"integer\"},\n",
    "                        \"L\": {\"type\": \"array\", \"items\": {\"type\": \"integer\"}, \"minItems\": 256, \"maxItems\": 256}\n",
    "                    },\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"rgb_stats\": {\"type\": \"object\"},\n",
    "                \"luma_stats\": {\"type\": \"object\"},\n",
    "                \"entropy\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"required\": [\"R\", \"G\", \"B\", \"L\"],\n",
    "                    \"properties\": {\n",
    "                        \"R\": {\"type\": [\"number\", \"null\"]},\n",
    "                        \"G\": {\"type\": [\"number\", \"null\"]},\n",
    "                        \"B\": {\"type\": [\"number\", \"null\"]},\n",
    "                        \"L\": {\"type\": [\"number\", \"null\"]}\n",
    "                    },\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"laplacian_var\": {\"type\": [\"number\", \"null\"]}\n",
    "            },\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"hashes\": {\n",
    "            \"type\": [\"object\", \"null\"],\n",
    "            \"properties\": {\"ahash\": {\"type\": \"string\"}, \"dhash\": {\"type\": \"string\"}, \"phash\": {\"type\": \"string\"}},\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "if not cpu_schema_path.exists():\n",
    "    cpu_schema_path.write_text(json.dumps(CPU_BASELINE_SCHEMA, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "cpu_validator = load_validator(cpu_schema_path)\n",
    "print(\"cpu.baseline.schema.json:\", cpu_schema_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"cpu-baseline-v1\"]\n",
    "NOTE = \"cpu baseline: container geometry hist entropy hashes laplacian\"\n",
    "RUN_ID_EFFECTIVE = RUN_ID or generate_run_id(MODELS, NOTE)\n",
    "\n",
    "report = {\n",
    "    \"run_id\": RUN_ID_EFFECTIVE,\n",
    "    \"timestamp\": utc_iso(),\n",
    "    \"dataset_root\": str(INPUT_ROOT),\n",
    "    \"input_root\": str(INPUT_ROOT),\n",
    "    \"output_root\": str(OUTPUT_ROOT),\n",
    "    \"shard_index\": SHARD_INDEX,\n",
    "    \"shard_count\": SHARD_COUNT,\n",
    "    \"plates_total\": len(plates),\n",
    "    \"plates_selected\": len(selected),\n",
    "    \"plates_processed\": 0,\n",
    "    \"plates_skipped\": 0,\n",
    "    \"decode_failures\": 0,\n",
    "    \"schema_failures\": 0,\n",
    "    \"errors_sample\": [],\n",
    "}\n",
    "\n",
    "def compute_cpu_baseline(plate_dir: Path) -> tuple[dict, dict]:\n",
    "    manifest = json.loads((plate_dir / \"manifest.json\").read_text(encoding=\"utf-8\"))\n",
    "    errs = list(plate_validator.iter_errors(manifest))\n",
    "    if errs:\n",
    "        raise RuntimeError(f\"plate manifest schema error: {list(errs[0].path)} -> {errs[0].message}\")\n",
    "\n",
    "    src_path = plate_dir / manifest[\"source_image\"]\n",
    "    if not src_path.exists():\n",
    "        raise RuntimeError(\"missing source image\")\n",
    "\n",
    "    run_manifest = {\n",
    "        \"run_id\": RUN_ID_EFFECTIVE,\n",
    "        \"plate_id\": plate_dir.name,\n",
    "        \"timestamp\": utc_iso(),\n",
    "        \"models\": MODELS,\n",
    "        \"outputs\": [\"cpu_baseline.json\"],\n",
    "        \"notes\": NOTE,\n",
    "    }\n",
    "\n",
    "    rerrs = list(run_validator.iter_errors(run_manifest))\n",
    "    if rerrs:\n",
    "        raise RuntimeError(f\"run manifest schema error: {list(rerrs[0].path)} -> {rerrs[0].message}\")\n",
    "\n",
    "    baseline = {\n",
    "        \"plate_id\": manifest[\"plate_id\"],\n",
    "        \"run_id\": RUN_ID_EFFECTIVE,\n",
    "        \"timestamp\": utc_iso(),\n",
    "        \"source_image\": manifest[\"source_image\"],\n",
    "        \"source_file\": {\n",
    "            \"bytes\": int(src_path.stat().st_size),\n",
    "            \"extension\": src_path.suffix.lower().lstrip(\".\"),\n",
    "            \"format\": None,\n",
    "            \"sha256\": sha256_file(src_path),\n",
    "            \"exif_present\": False,\n",
    "            \"icc_present\": False,\n",
    "            \"icc_hash\": None,\n",
    "            \"jpeg_is_progressive\": None,\n",
    "            \"jpeg_subsampling\": None,\n",
    "            \"jpeg_quant_hash\": None,\n",
    "        },\n",
    "        \"geometry\": {\"width_px\": None, \"height_px\": None, \"megapixels\": None, \"aspect_ratio\": None, \"mode\": None},\n",
    "        \"tiling\": {\"tile_size_px\": TILE_SIZE, \"tiles_x\": 1, \"tiles_y\": 1, \"total_tiles\": 1},\n",
    "        \"decode\": {\"ok\": False, \"error\": None},\n",
    "        \"pixel_stats\": None,\n",
    "        \"hashes\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with Image.open(src_path) as img:\n",
    "            baseline[\"source_file\"][\"format\"] = img.format\n",
    "            baseline[\"source_file\"][\"exif_present\"] = exif_present(img)\n",
    "            ip, ih = icc_hash(img)\n",
    "            baseline[\"source_file\"][\"icc_present\"] = ip\n",
    "            baseline[\"source_file\"][\"icc_hash\"] = ih\n",
    "            if (img.format or \"\").upper() == \"JPEG\":\n",
    "                baseline[\"source_file\"][\"jpeg_is_progressive\"] = jpeg_progressive_flag(img)\n",
    "                baseline[\"source_file\"][\"jpeg_subsampling\"] = jpeg_subsampling_flag(img)\n",
    "                baseline[\"source_file\"][\"jpeg_quant_hash\"] = quant_hash_from_pil(img)\n",
    "\n",
    "            w, h = img.size\n",
    "            baseline[\"geometry\"] = {\n",
    "                \"width_px\": int(w),\n",
    "                \"height_px\": int(h),\n",
    "                \"megapixels\": float(round((w * h) / 1_000_000, 3)),\n",
    "                \"aspect_ratio\": float(w / h),\n",
    "                \"mode\": img.mode,\n",
    "            }\n",
    "\n",
    "            tiles_x = (w + TILE_SIZE - 1) // TILE_SIZE\n",
    "            tiles_y = (h + TILE_SIZE - 1) // TILE_SIZE\n",
    "            baseline[\"tiling\"] = {\n",
    "                \"tile_size_px\": TILE_SIZE,\n",
    "                \"tiles_x\": int(tiles_x),\n",
    "                \"tiles_y\": int(tiles_y),\n",
    "                \"total_tiles\": int(tiles_x * tiles_y),\n",
    "            }\n",
    "\n",
    "            rgb = img.convert(\"RGB\")\n",
    "            raw = rgb.histogram()\n",
    "            Rh, Gh, Bh = raw[0:256], raw[256:512], raw[512:768]\n",
    "            L = rgb.convert(\"L\")\n",
    "            Lh = L.histogram()\n",
    "\n",
    "            baseline[\"pixel_stats\"] = {\n",
    "                \"colorspace\": \"as-decoded\",\n",
    "                \"rgb_histograms\": {\"bins\": HIST_BINS, \"R\": Rh, \"G\": Gh, \"B\": Bh},\n",
    "                \"l_histogram\": {\"bins\": HIST_BINS, \"L\": Lh},\n",
    "                \"rgb_stats\": {\"R\": hist_stats(Rh), \"G\": hist_stats(Gh), \"B\": hist_stats(Bh)},\n",
    "                \"luma_stats\": hist_stats(Lh),\n",
    "                \"entropy\": {\"R\": entropy(Rh), \"G\": entropy(Gh), \"B\": entropy(Bh), \"L\": entropy(Lh)},\n",
    "                \"laplacian_var\": laplacian_variance(L),\n",
    "            }\n",
    "            baseline[\"hashes\"] = {\"ahash\": ahash(rgb), \"dhash\": dhash(rgb), \"phash\": phash(rgb)}\n",
    "            baseline[\"decode\"] = {\"ok\": True, \"error\": None}\n",
    "\n",
    "    except Exception as e:\n",
    "        baseline[\"decode\"] = {\"ok\": False, \"error\": f\"{type(e).__name__}: {str(e)[:300]}\"}\n",
    "\n",
    "    berrs = list(cpu_validator.iter_errors(baseline))\n",
    "    if berrs:\n",
    "        raise RuntimeError(f\"cpu baseline schema error: {list(berrs[0].path)} -> {berrs[0].message}\")\n",
    "\n",
    "    return run_manifest, baseline\n",
    "\n",
    "for plate_dir in tqdm(selected, desc=\"plates\"):\n",
    "    out_plate_dir = OUTPUT_ROOT / \"plates_structured\" / plate_dir.name\n",
    "    out_run_dir = out_plate_dir / \"runs\" / RUN_ID_EFFECTIVE\n",
    "    out_run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_metrics = out_run_dir / \"metrics.json\"\n",
    "    out_cpu = out_run_dir / \"cpu_baseline.json\"\n",
    "\n",
    "    if SKIP_IF_PRESENT and out_metrics.exists() and out_cpu.exists():\n",
    "        report[\"plates_skipped\"] += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        run_manifest, baseline = compute_cpu_baseline(plate_dir)\n",
    "        out_metrics.write_text(json.dumps(run_manifest, indent=2), encoding=\"utf-8\")\n",
    "        out_cpu.write_text(json.dumps(baseline, indent=2), encoding=\"utf-8\")\n",
    "        report[\"plates_processed\"] += 1\n",
    "    except Exception as e:\n",
    "        report[\"schema_failures\"] += 1\n",
    "        if len(report[\"errors_sample\"]) < 10:\n",
    "            report[\"errors_sample\"].append(f\"{plate_dir.name}: {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "report_dir = OUTPUT_ROOT / \"reports\" / RUN_ID_EFFECTIVE\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "(report_dir / \"report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
